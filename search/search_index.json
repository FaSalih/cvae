{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"cvae Chemical Variational Autoencoder. Based off this great project. API Documentation Install Set up env git clone https://github.com/ghsanti/cvae cvae #clones into `cvae` folder # replace clone by fork, if you plan to develop cd cvae mamba create -n cvae \"python>=3.11.9,<3.12\" mamba init && source ~/.bashrc mamba activate cvae #check your python version now (if wrong, upgrade it.) python --version Install dependencies mamba install poetry graphviz -c conda-forge mamba install nodejs -c conda-forge # only for development # poetry will install deps and the project respecting conda. # this also allows running the notebooks and scripts. poetry install --without dev --sync # unless you want dev deps. # if poetry fails run the one below # poetry install --only-main Run a training: You can modify the program's configuration within scripts/train_vae.py python scripts/train_vae.py or python -m scripts.train_vae Updates Refactored. Uses Keras v3 Removed TerminalGRU (this may result in less precision.) Supported backends: Tensorflow, Pytorch, Jax. Typed. CI. As long as a layer only uses APIs from the keras.ops namespace (or other Keras namespaces such as keras.activations , keras.random , or keras.layers ), then it can be used with any backend \u2013 TensorFlow, JAX, or PyTorch. By not using backend specifics, it turns the code to multibackend. Source: Keras. Upcoming Currently only the encoder+decoder was tested, not the property predictor. We will test it. Train with more accuracy (and upload best weights.) Make it run in a web browser. Use it to suggest new molecules with specific properties. Update notebooks Attributions This is a refactor / fork of the original chemical VAE by Aspuru-Guzik's Group Part of the original readme (updated) is below (includes authors.) This repository contains the framework and code for constructing a variational autoencoder (VAE) for use with molecular SMILES, as described in doi:10.1021/acscentsci.7b00572 , with preprint at https://arxiv.org/pdf/1610.02415.pdf . In short, molecular SMILES are encoded into a code vector representation, and can be decoded from the code representation back to molecular SMILES. The autoencoder may also be jointly trained with property prediction to help shape the latent space. The new latent space can then be optimized upon to find the molecules with the most optimized properties of interest. In our example, we perform encoding/decoding with the ZINC dataset, and shape the latent space on prediction on logP, QED, and SAS properties. Jupyter notebook is required to run the ipynb examples. Make sure that the Keras backend is set to use Tensorflow Components scripts/train_vae.py : main script for training variational autoencoder models.py - Library of models, contains the encoder, decoder and property prediction models. tgru_k2_gpu.py - Custom keras layer containing custom teacher forcing/sampling sampled_rnn_tf.py - Custom rnn function for tgru_k2_gpu.py, written in tensorflow backend. hyperparameters/user - Configuration. mol_utils.py - library for parsing SMILES into one-hot encoding and vice versa mol_callbacks.py - library containing callbacks used by train_vae.py Includes Weight_Annealer callback, which is used to update the weight of the KL loss component vae_utils.py - utility functions for an autoencoder object, used post processing. Authors: This software is written by Jennifer Wei, Benjamin Sanchez-Lengeling, Dennis Sheberla, Rafael Gomez-Bomberelli, and Alan Aspuru-Guzik (alan@aspuru.com). It is based on the work published in https://arxiv.org/pdf/1610.02415.pdf by Rafa G\u00f3mez-Bombarelli , Jennifer Wei , David Duvenaud , Jos\u00e9 Miguel Hern\u00e1ndez-Lobato , Benjam\u00edn S\u00e1nchez-Lengeling , Dennis Sheberla , Jorge Aguilera-Iparraguirre , Timothy Hirzel , Ryan P. Adams , Al\u00e1n Aspuru-Guzik","title":"cvae"},{"location":"#cvae","text":"Chemical Variational Autoencoder. Based off this great project.","title":"cvae"},{"location":"#api-documentation","text":"","title":"API Documentation"},{"location":"#install","text":"Set up env git clone https://github.com/ghsanti/cvae cvae #clones into `cvae` folder # replace clone by fork, if you plan to develop cd cvae mamba create -n cvae \"python>=3.11.9,<3.12\" mamba init && source ~/.bashrc mamba activate cvae #check your python version now (if wrong, upgrade it.) python --version Install dependencies mamba install poetry graphviz -c conda-forge mamba install nodejs -c conda-forge # only for development # poetry will install deps and the project respecting conda. # this also allows running the notebooks and scripts. poetry install --without dev --sync # unless you want dev deps. # if poetry fails run the one below # poetry install --only-main Run a training: You can modify the program's configuration within scripts/train_vae.py python scripts/train_vae.py or python -m scripts.train_vae","title":"Install"},{"location":"#updates","text":"Refactored. Uses Keras v3 Removed TerminalGRU (this may result in less precision.) Supported backends: Tensorflow, Pytorch, Jax. Typed. CI. As long as a layer only uses APIs from the keras.ops namespace (or other Keras namespaces such as keras.activations , keras.random , or keras.layers ), then it can be used with any backend \u2013 TensorFlow, JAX, or PyTorch. By not using backend specifics, it turns the code to multibackend. Source: Keras.","title":"Updates"},{"location":"#upcoming","text":"Currently only the encoder+decoder was tested, not the property predictor. We will test it. Train with more accuracy (and upload best weights.) Make it run in a web browser. Use it to suggest new molecules with specific properties. Update notebooks","title":"Upcoming"},{"location":"#attributions","text":"This is a refactor / fork of the original chemical VAE by Aspuru-Guzik's Group Part of the original readme (updated) is below (includes authors.) This repository contains the framework and code for constructing a variational autoencoder (VAE) for use with molecular SMILES, as described in doi:10.1021/acscentsci.7b00572 , with preprint at https://arxiv.org/pdf/1610.02415.pdf . In short, molecular SMILES are encoded into a code vector representation, and can be decoded from the code representation back to molecular SMILES. The autoencoder may also be jointly trained with property prediction to help shape the latent space. The new latent space can then be optimized upon to find the molecules with the most optimized properties of interest. In our example, we perform encoding/decoding with the ZINC dataset, and shape the latent space on prediction on logP, QED, and SAS properties. Jupyter notebook is required to run the ipynb examples. Make sure that the Keras backend is set to use Tensorflow","title":"Attributions"},{"location":"#components","text":"scripts/train_vae.py : main script for training variational autoencoder models.py - Library of models, contains the encoder, decoder and property prediction models. tgru_k2_gpu.py - Custom keras layer containing custom teacher forcing/sampling sampled_rnn_tf.py - Custom rnn function for tgru_k2_gpu.py, written in tensorflow backend. hyperparameters/user - Configuration. mol_utils.py - library for parsing SMILES into one-hot encoding and vice versa mol_callbacks.py - library containing callbacks used by train_vae.py Includes Weight_Annealer callback, which is used to update the weight of the KL loss component vae_utils.py - utility functions for an autoencoder object, used post processing.","title":"Components"},{"location":"#authors","text":"This software is written by Jennifer Wei, Benjamin Sanchez-Lengeling, Dennis Sheberla, Rafael Gomez-Bomberelli, and Alan Aspuru-Guzik (alan@aspuru.com). It is based on the work published in https://arxiv.org/pdf/1610.02415.pdf by Rafa G\u00f3mez-Bombarelli , Jennifer Wei , David Duvenaud , Jos\u00e9 Miguel Hern\u00e1ndez-Lobato , Benjam\u00edn S\u00e1nchez-Lengeling , Dennis Sheberla , Jorge Aguilera-Iparraguirre , Timothy Hirzel , Ryan P. Adams , Al\u00e1n Aspuru-Guzik","title":"Authors:"},{"location":"default_config.py/","text":"User configuration. Classes Config(data_dir: Path) Set your configuration here. Define the parameters. Ideally leave these below as default, change only below \"USER\". Source code in chemvae/default_config.py def __init__(self, data_dir: Path): \"\"\"Define the parameters. Ideally leave these below as default, change only below \"USER\". \"\"\" # FILES self.checkpoint_path: str = \"./\" self.data_normalization_out_file: str | None = None self.data_file: str = \"qm9.csv\" self.char_file: str = \"qm9.json\" self.encoder_weights_file: str = \"encoder_2.keras\" self.decoder_weights_file: str = \"decoder_2.keras\" self.prop_pred_weights_file: str = \"qm9_prop_pred.keras\" self.test_idx_file: str = \"test_idx.npy\" self.history_file: str = \"history_4.csv\" # GENERAL PARAMETERS self.batch_size: int = 100 self.epochs: int = 70 self.batch_size: int = 126 self.latent_dim: int = 156 # 196 self.val_split: float = 0.1 # validation split self.loss: str = \"categorical_crossentropy\" # set reconstruction loss self.do_prop_pred: bool = False self.MAX_LEN: int = 34 # 120 self.TRAIN_MODEL: bool = True self.ENC_DEC_TEST: bool = False self.PADDING: Literal[\"right\", \"left\", \"none\"] = \"right\" self.RAND_SEED: int = 42 # OPTIMIZATION PARAMETERS self.lr: float = 0.000312087049936 self.momentum: float = 0.936948773087 self.optim: str = \"adam\" # optimizer to be used # CONVOLUTION PARAMETERS self.batchnorm_conv: bool = True # block: Conv+Batch Norm. But BN can be disabled above. self.conv_layer_blocks: int = 3 # conv depth self.conv_filters: int = 5 self.conv_filter_growth_factor: float = 0.5 self.conv_kernel_height: int = 6 self.conv_kernel_growth_factor: float = 0.85 self.conv_activation: str = \"tanh\" # MIDDLE LAYER PARAMETERS # growth factor applied to determine size of next middle layer self.hg_growth_factor: float = 1.4928245388 self.middle_layer: int = 1 self.dropout_rate_mid: float = 0.0 self.batchnorm_mid: bool = True # apply batch normalization to middle layer self.activation: str = \"tanh\" # DECODER PARAMETERS self.gru_depth: int = 4 self.rnn_activation: str = \"tanh\" self.recurrent_dim: int = 50 # use CPU intensive implementation; other implementation modes # (1 - GPU 2- memory) are not yet implemented self.temperature: float = 1.00 # amount of noise for sampling the final output # VAE PARAMETERS self.vae_annealer_start: int = ( 22 # Center for variational weight annealer. What is that? ) self.batchnorm_vae: bool = ( False # apply batch normalization to output of the variational layer ) self.vae_activation: str = \"tanh\" self.xent_loss_weight: float = ( 1.0 # loss weight to assign to reconstruction error ) self.kl_loss_weight: float = 1.0 # loss weight to assign to KL loss self.anneal_sigmod_slope: float = ( 1.0 # slope of sigmoid variational weight annealer ) # Choice of freezing the variational layer until close to the anneal starting epoch self.freeze_logvar_layer: bool = False # the number of epochs before vae_annealer_start where the variational layer should be unfrozen self.freeze_offset: int = 1 # PROPERTY PREDICTION PARAMETERS self.reg_prop_tasks: list[str] | None = None self.logit_prop_tasks: list[str] | None = None # path to write out normalized regression data (or None) # ratio between consecutive layers in property pred. self.prop_pred_depth: int = 3 self.prop_hidden_dim: int = 36 self.prop_growth_factor: float = 0.8 self.prop_pred_activation: str = \"tanh\" # loss function to use with property prediction error for regression task self.reg_prop_pred_loss: str = \"mse\" # loss function to use with property prediction for logistic task self.logit_prop_pred_loss: str = \"binary_crossentropy\" self.prop_pred_loss_weight: float = 0.5 self.prop_pred_dropout: float = 0.0 self.prop_batchnorm: bool = True self.verbosity: int = 1 self.limit_data: int | None = None self.reload_model: bool = False # startmodel from checkpoint self.prev_epochs: int = 0 self.vae_annealer_start: int = 22 # 29 # where does this come from? self.dropout_rate_mid: float = 0.082832929704794792 self.anneal_sigmod_slope: float = 0.51066543057913916 self.recurrent_dim: int = 488 self.hg_growth_factor: float = 1.2281884874932403 self.middle_layer: int = 1 # concat and make them paths for k, v in list(vars(self).items()): if k.endswith(\"file\") and isinstance(v, str): setattr(self, k, data_dir.joinpath(v).resolve()) elif k.endswith(\"path\"): setattr(self, k, data_dir.joinpath(v).resolve()) # load chars and set them self.CHARS: str = yaml.safe_load(open(self.char_file)) self.NCHARS: int = len(self.CHARS) Functions","title":"Default config.py"},{"location":"default_config.py/#chemvae.default_config-classes","text":"","title":"Classes"},{"location":"default_config.py/#chemvae.default_config.Config","text":"Set your configuration here. Define the parameters. Ideally leave these below as default, change only below \"USER\". Source code in chemvae/default_config.py def __init__(self, data_dir: Path): \"\"\"Define the parameters. Ideally leave these below as default, change only below \"USER\". \"\"\" # FILES self.checkpoint_path: str = \"./\" self.data_normalization_out_file: str | None = None self.data_file: str = \"qm9.csv\" self.char_file: str = \"qm9.json\" self.encoder_weights_file: str = \"encoder_2.keras\" self.decoder_weights_file: str = \"decoder_2.keras\" self.prop_pred_weights_file: str = \"qm9_prop_pred.keras\" self.test_idx_file: str = \"test_idx.npy\" self.history_file: str = \"history_4.csv\" # GENERAL PARAMETERS self.batch_size: int = 100 self.epochs: int = 70 self.batch_size: int = 126 self.latent_dim: int = 156 # 196 self.val_split: float = 0.1 # validation split self.loss: str = \"categorical_crossentropy\" # set reconstruction loss self.do_prop_pred: bool = False self.MAX_LEN: int = 34 # 120 self.TRAIN_MODEL: bool = True self.ENC_DEC_TEST: bool = False self.PADDING: Literal[\"right\", \"left\", \"none\"] = \"right\" self.RAND_SEED: int = 42 # OPTIMIZATION PARAMETERS self.lr: float = 0.000312087049936 self.momentum: float = 0.936948773087 self.optim: str = \"adam\" # optimizer to be used # CONVOLUTION PARAMETERS self.batchnorm_conv: bool = True # block: Conv+Batch Norm. But BN can be disabled above. self.conv_layer_blocks: int = 3 # conv depth self.conv_filters: int = 5 self.conv_filter_growth_factor: float = 0.5 self.conv_kernel_height: int = 6 self.conv_kernel_growth_factor: float = 0.85 self.conv_activation: str = \"tanh\" # MIDDLE LAYER PARAMETERS # growth factor applied to determine size of next middle layer self.hg_growth_factor: float = 1.4928245388 self.middle_layer: int = 1 self.dropout_rate_mid: float = 0.0 self.batchnorm_mid: bool = True # apply batch normalization to middle layer self.activation: str = \"tanh\" # DECODER PARAMETERS self.gru_depth: int = 4 self.rnn_activation: str = \"tanh\" self.recurrent_dim: int = 50 # use CPU intensive implementation; other implementation modes # (1 - GPU 2- memory) are not yet implemented self.temperature: float = 1.00 # amount of noise for sampling the final output # VAE PARAMETERS self.vae_annealer_start: int = ( 22 # Center for variational weight annealer. What is that? ) self.batchnorm_vae: bool = ( False # apply batch normalization to output of the variational layer ) self.vae_activation: str = \"tanh\" self.xent_loss_weight: float = ( 1.0 # loss weight to assign to reconstruction error ) self.kl_loss_weight: float = 1.0 # loss weight to assign to KL loss self.anneal_sigmod_slope: float = ( 1.0 # slope of sigmoid variational weight annealer ) # Choice of freezing the variational layer until close to the anneal starting epoch self.freeze_logvar_layer: bool = False # the number of epochs before vae_annealer_start where the variational layer should be unfrozen self.freeze_offset: int = 1 # PROPERTY PREDICTION PARAMETERS self.reg_prop_tasks: list[str] | None = None self.logit_prop_tasks: list[str] | None = None # path to write out normalized regression data (or None) # ratio between consecutive layers in property pred. self.prop_pred_depth: int = 3 self.prop_hidden_dim: int = 36 self.prop_growth_factor: float = 0.8 self.prop_pred_activation: str = \"tanh\" # loss function to use with property prediction error for regression task self.reg_prop_pred_loss: str = \"mse\" # loss function to use with property prediction for logistic task self.logit_prop_pred_loss: str = \"binary_crossentropy\" self.prop_pred_loss_weight: float = 0.5 self.prop_pred_dropout: float = 0.0 self.prop_batchnorm: bool = True self.verbosity: int = 1 self.limit_data: int | None = None self.reload_model: bool = False # startmodel from checkpoint self.prev_epochs: int = 0 self.vae_annealer_start: int = 22 # 29 # where does this come from? self.dropout_rate_mid: float = 0.082832929704794792 self.anneal_sigmod_slope: float = 0.51066543057913916 self.recurrent_dim: int = 488 self.hg_growth_factor: float = 1.2281884874932403 self.middle_layer: int = 1 # concat and make them paths for k, v in list(vars(self).items()): if k.endswith(\"file\") and isinstance(v, str): setattr(self, k, data_dir.joinpath(v).resolve()) elif k.endswith(\"path\"): setattr(self, k, data_dir.joinpath(v).resolve()) # load chars and set them self.CHARS: str = yaml.safe_load(open(self.char_file)) self.NCHARS: int = len(self.CHARS)","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;Config"},{"location":"default_config.py/#chemvae.default_config.Config-functions","text":"","title":"Functions"},{"location":"models.py/","text":"Encoder,Decoder, and Predictor. Classes Sampling(kl_loss_var: Variable, rand_seed: int, **kwargs) Bases: Layer Uses (z_mean, z_log_var) to sample z, the vector encoding a smile. Configure Sampling. kl_loss_var: a float that will tend to zero as epochs pass. It is recalculated on each epoch end. rand_seed: random seed Source code in chemvae/models.py def __init__(self, kl_loss_var: Variable, rand_seed: int, **kwargs): \"\"\"Configure Sampling. kl_loss_var: a float that will tend to zero as epochs pass. It is recalculated on each epoch end. rand_seed: random seed \"\"\" super().__init__(**kwargs) self.seed_generator = keras.random.SeedGenerator(rand_seed) self.kl_loss_var = kl_loss_var Functions compute_output_shape(input_shape) Compute shape. Keras isn't able to infer it. Source code in chemvae/models.py def compute_output_shape(self, input_shape): \"\"\"Compute shape. Keras isn't able to infer it.\"\"\" return input_shape[0] Functions decoder_model(params: Config) -> Functional Create decoder model. Retrieves a SMILES tensor from vector. Source code in chemvae/models.py def decoder_model(params: Config) -> Functional: \"\"\"Create decoder model. Retrieves a SMILES tensor from vector.\"\"\" # sanity checks if params.middle_layer < 0: raise Exception(\"params.middle_layer must be >=0\") elif not isinstance(params.gru_depth, int): raise Exception(\"params.gru_depth must be an integer>0.\") z_in = Input(shape=(params.latent_dim,), name=\"decoder_input\") z = z_in for i in range(params.middle_layer): # this is un-bottleneck / expand vector. z = Dense( units=int(params.latent_dim * params.hg_growth_factor ** (i)), activation=params.activation, name=f\"decoder_dense{i}\", )(z) if params.dropout_rate_mid > 0: z = Dropout(rate=params.dropout_rate_mid)(z) if params.batchnorm_mid: z = BatchNormalization(axis=-1, name=f\"decoder_dense{i}_norm\")(z) # Necessary for using GRU vectors # expand to original row-size z_reps = RepeatVector(n=params.MAX_LEN)(z) x_out = z_reps if params.gru_depth >= 2: for i in range(params.gru_depth - 1): # if 2, runs once x_out = GRU( units=params.recurrent_dim, return_sequences=True, activation=\"tanh\", name=f\"decoder_gru{i}\", )(x_out) x_out = GRU( units=params.NCHARS, return_sequences=True, activation=\"softmax\", name=\"decoder_gru_final\", )(x_out) return Model(z_in, x_out, name=\"decoder\") encoder_model(params: Config) -> Functional Define hot-encoded smile to vector processing. params: program and network configuration. Returns: encoder model Conv Blocks Flatten Dense Blocks middle (nn 1) middle->Dense Layer-> z (nn 2) Source code in chemvae/models.py def encoder_model(params: Config) -> Functional: \"\"\"Define hot-encoded smile to vector processing. params: program and network configuration. Returns: encoder model 1. Conv Blocks 2. Flatten 3. Dense Blocks 4. middle (nn 1) 5. middle->Dense Layer-> z (nn 2) \"\"\" # sanity check if params.middle_layer < 0: raise Exception(\"params.middle_layer must be >=0\") # smiles is the batch dimension. # starts the symbolic descriotion x_in = Input(shape=(params.MAX_LEN, params.NCHARS), name=\"input_molecule_smi\") # Convolution layers x = x_in # they seem to do it in keras docs as well. # note that `x` is returned anew, not mutated, below. # 1D Convolutions # kernel_size=kernel rows, the cols are same as input. for j in range(params.conv_layer_blocks): x = Convolution1D( filters=int( params.conv_filters * params.conv_filter_growth_factor ** (j or 1) ), kernel_size=int( params.conv_kernel_height * params.conv_kernel_growth_factor ** (j or 1) ), activation=\"tanh\", name=\"encoder_conv{}\".format(j), )(x) if params.batchnorm_conv: x = BatchNormalization(axis=-1, name=\"encoder_norm{}\".format(j))(x) x = Flatten()(x) # flatten in order to use the Dense layers. # from here to the output, 1D vector (+ batch dimension) # Middle layers # Dense->Dropout->Batchnorm pattern. shared = x for i in range(params.middle_layer): # note that this shinks, it's a bottleneck shared = Dense( units=int( # a fn of the latent dim params.latent_dim * params.hg_growth_factor ** (params.middle_layer - i) ), activation=params.activation, name=\"encoder_dense{}\".format(i), )(shared) # what happens with this in inference time? if params.dropout_rate_mid > 0: shared = Dropout(params.dropout_rate_mid)(shared) if params.batchnorm_mid: shared = BatchNormalization(axis=-1, name=f\"encoder_dense{i}_norm\")(shared) z_mean = Dense(units=params.latent_dim, name=\"z_mean\")(shared) # return z & previous encoding layer for std dev sampling return Model(inputs=x_in, outputs=[z_mean, shared], name=\"encoder\") load_decoder(params: Config) Load decoder model weights file. Source code in chemvae/models.py def load_decoder(params: Config): \"\"\"Load decoder model weights file.\"\"\" return cast(Functional, load_model(params.decoder_weights_file)) load_encoder(params: Config) Reload a saved encoder. Source code in chemvae/models.py def load_encoder(params: Config): \"\"\"Reload a saved encoder.\"\"\" return cast(Functional, load_model(params.encoder_weights_file)) load_property_predictor(params) Load the property predictor network from file path. Source code in chemvae/models.py def load_property_predictor(params): \"\"\"Load the property predictor network from file path.\"\"\" return cast(Functional, load_model(params.prop_pred_weights_file)) property_predictor_model(params: Config) -> Functional One or two independent property predictors. params: Program and Network configuration. Composed of: Dense Blocks. They share the inner layers, but not the out layer. Source code in chemvae/models.py def property_predictor_model(params: Config) -> Functional: \"\"\"One or two independent property predictors. params: Program and Network configuration. Composed of: Dense Blocks. They share the inner layers, but not the out layer. \"\"\" reg_prop_tasks: list[str] | None = params.reg_prop_tasks logit_prop_tasks: list[str] | None = params.logit_prop_tasks # the input to this model is z-dim, not including the batch dimension. ls_in = Input(shape=(params.latent_dim,), name=\"prop_pred_input\") prop_mid = ls_in # shared weights part for p_i in range(params.prop_pred_depth): prop_mid = Dense( int(params.prop_hidden_dim * params.prop_growth_factor**p_i), activation=params.prop_pred_activation, name=f\"property_predictor_dense{p_i}\", )(prop_mid) if params.prop_pred_dropout > 0: prop_mid = Dropout(params.prop_pred_dropout)(prop_mid) if params.prop_batchnorm: prop_mid = BatchNormalization()(prop_mid) # for regression tasks logit_prop_pred = None reg_prop_pred = None if isinstance(reg_prop_tasks, list) and len(reg_prop_tasks) > 0: reg_prop_pred = Dense( units=len(reg_prop_tasks), activation=\"linear\", # linear for the output, since may be - name=\"reg_property_output\", )(prop_mid) # for logistic tasks if isinstance(logit_prop_tasks, list) and len(logit_prop_tasks) > 0: logit_prop_pred = Dense( units=len(logit_prop_tasks), activation=\"sigmoid\", # may be better softmax name=\"logit_property_output\", )(prop_mid) # both regression and logistic if logit_prop_pred and reg_prop_pred: return Model(ls_in, [reg_prop_pred, logit_prop_pred], name=\"property_predictor\") # regression only scenario elif reg_prop_pred: return Model(ls_in, reg_prop_pred, name=\"property_predictor\") # logit only scenario elif logit_prop_pred: return Model(ls_in, logit_prop_pred, name=\"property_predictor\") raise Exception( \"No model was created. To run property prediction add the tasks' list in the configuration.\" ) sample_latent_vector(z_mean, shared, kl_loss_var: Variable, params: Config) Create variational layers. Adds noise to z. z_mean: from encoder's Dense()(z). z_log_var: forks into z_mean and z_mean_log_var kl_loss_var: ... params: parameter dictionary passed throughout entire model. Source code in chemvae/models.py def sample_latent_vector(z_mean, shared, kl_loss_var: Variable, params: Config): \"\"\"Create variational layers. Adds noise to z. z_mean: from encoder's Dense()(z). z_log_var: forks into z_mean and z_mean_log_var kl_loss_var: ... params: parameter dictionary passed throughout entire model. \"\"\" # why concatenate? z_log_var = Dense(units=params.latent_dim, name=\"z_log_var\")(shared) z_mean_z_log_var_output = Concatenate(name=\"z_mean_z_log_var\")([z_mean, z_log_var]) z_samp = Sampling( kl_loss_var=kl_loss_var, rand_seed=params.RAND_SEED, name=\"z_samp\", )([z_mean, z_log_var]) if params.batchnorm_vae: z_samp = BatchNormalization(axis=-1)(z_samp) return z_samp, z_mean_z_log_var_output","title":"Models.py"},{"location":"models.py/#chemvae.models-classes","text":"","title":"Classes"},{"location":"models.py/#chemvae.models.Sampling","text":"Bases: Layer Uses (z_mean, z_log_var) to sample z, the vector encoding a smile. Configure Sampling. kl_loss_var: a float that will tend to zero as epochs pass. It is recalculated on each epoch end. rand_seed: random seed Source code in chemvae/models.py def __init__(self, kl_loss_var: Variable, rand_seed: int, **kwargs): \"\"\"Configure Sampling. kl_loss_var: a float that will tend to zero as epochs pass. It is recalculated on each epoch end. rand_seed: random seed \"\"\" super().__init__(**kwargs) self.seed_generator = keras.random.SeedGenerator(rand_seed) self.kl_loss_var = kl_loss_var","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;Sampling"},{"location":"models.py/#chemvae.models.Sampling-functions","text":"","title":"Functions"},{"location":"models.py/#chemvae.models.Sampling.compute_output_shape","text":"Compute shape. Keras isn't able to infer it. Source code in chemvae/models.py def compute_output_shape(self, input_shape): \"\"\"Compute shape. Keras isn't able to infer it.\"\"\" return input_shape[0]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;compute_output_shape"},{"location":"models.py/#chemvae.models-functions","text":"","title":"Functions"},{"location":"models.py/#chemvae.models.decoder_model","text":"Create decoder model. Retrieves a SMILES tensor from vector. Source code in chemvae/models.py def decoder_model(params: Config) -> Functional: \"\"\"Create decoder model. Retrieves a SMILES tensor from vector.\"\"\" # sanity checks if params.middle_layer < 0: raise Exception(\"params.middle_layer must be >=0\") elif not isinstance(params.gru_depth, int): raise Exception(\"params.gru_depth must be an integer>0.\") z_in = Input(shape=(params.latent_dim,), name=\"decoder_input\") z = z_in for i in range(params.middle_layer): # this is un-bottleneck / expand vector. z = Dense( units=int(params.latent_dim * params.hg_growth_factor ** (i)), activation=params.activation, name=f\"decoder_dense{i}\", )(z) if params.dropout_rate_mid > 0: z = Dropout(rate=params.dropout_rate_mid)(z) if params.batchnorm_mid: z = BatchNormalization(axis=-1, name=f\"decoder_dense{i}_norm\")(z) # Necessary for using GRU vectors # expand to original row-size z_reps = RepeatVector(n=params.MAX_LEN)(z) x_out = z_reps if params.gru_depth >= 2: for i in range(params.gru_depth - 1): # if 2, runs once x_out = GRU( units=params.recurrent_dim, return_sequences=True, activation=\"tanh\", name=f\"decoder_gru{i}\", )(x_out) x_out = GRU( units=params.NCHARS, return_sequences=True, activation=\"softmax\", name=\"decoder_gru_final\", )(x_out) return Model(z_in, x_out, name=\"decoder\")","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;decoder_model"},{"location":"models.py/#chemvae.models.encoder_model","text":"Define hot-encoded smile to vector processing. params: program and network configuration. Returns: encoder model Conv Blocks Flatten Dense Blocks middle (nn 1) middle->Dense Layer-> z (nn 2) Source code in chemvae/models.py def encoder_model(params: Config) -> Functional: \"\"\"Define hot-encoded smile to vector processing. params: program and network configuration. Returns: encoder model 1. Conv Blocks 2. Flatten 3. Dense Blocks 4. middle (nn 1) 5. middle->Dense Layer-> z (nn 2) \"\"\" # sanity check if params.middle_layer < 0: raise Exception(\"params.middle_layer must be >=0\") # smiles is the batch dimension. # starts the symbolic descriotion x_in = Input(shape=(params.MAX_LEN, params.NCHARS), name=\"input_molecule_smi\") # Convolution layers x = x_in # they seem to do it in keras docs as well. # note that `x` is returned anew, not mutated, below. # 1D Convolutions # kernel_size=kernel rows, the cols are same as input. for j in range(params.conv_layer_blocks): x = Convolution1D( filters=int( params.conv_filters * params.conv_filter_growth_factor ** (j or 1) ), kernel_size=int( params.conv_kernel_height * params.conv_kernel_growth_factor ** (j or 1) ), activation=\"tanh\", name=\"encoder_conv{}\".format(j), )(x) if params.batchnorm_conv: x = BatchNormalization(axis=-1, name=\"encoder_norm{}\".format(j))(x) x = Flatten()(x) # flatten in order to use the Dense layers. # from here to the output, 1D vector (+ batch dimension) # Middle layers # Dense->Dropout->Batchnorm pattern. shared = x for i in range(params.middle_layer): # note that this shinks, it's a bottleneck shared = Dense( units=int( # a fn of the latent dim params.latent_dim * params.hg_growth_factor ** (params.middle_layer - i) ), activation=params.activation, name=\"encoder_dense{}\".format(i), )(shared) # what happens with this in inference time? if params.dropout_rate_mid > 0: shared = Dropout(params.dropout_rate_mid)(shared) if params.batchnorm_mid: shared = BatchNormalization(axis=-1, name=f\"encoder_dense{i}_norm\")(shared) z_mean = Dense(units=params.latent_dim, name=\"z_mean\")(shared) # return z & previous encoding layer for std dev sampling return Model(inputs=x_in, outputs=[z_mean, shared], name=\"encoder\")","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;encoder_model"},{"location":"models.py/#chemvae.models.load_decoder","text":"Load decoder model weights file. Source code in chemvae/models.py def load_decoder(params: Config): \"\"\"Load decoder model weights file.\"\"\" return cast(Functional, load_model(params.decoder_weights_file))","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_decoder"},{"location":"models.py/#chemvae.models.load_encoder","text":"Reload a saved encoder. Source code in chemvae/models.py def load_encoder(params: Config): \"\"\"Reload a saved encoder.\"\"\" return cast(Functional, load_model(params.encoder_weights_file))","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_encoder"},{"location":"models.py/#chemvae.models.load_property_predictor","text":"Load the property predictor network from file path. Source code in chemvae/models.py def load_property_predictor(params): \"\"\"Load the property predictor network from file path.\"\"\" return cast(Functional, load_model(params.prop_pred_weights_file))","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_property_predictor"},{"location":"models.py/#chemvae.models.property_predictor_model","text":"One or two independent property predictors. params: Program and Network configuration. Composed of: Dense Blocks. They share the inner layers, but not the out layer. Source code in chemvae/models.py def property_predictor_model(params: Config) -> Functional: \"\"\"One or two independent property predictors. params: Program and Network configuration. Composed of: Dense Blocks. They share the inner layers, but not the out layer. \"\"\" reg_prop_tasks: list[str] | None = params.reg_prop_tasks logit_prop_tasks: list[str] | None = params.logit_prop_tasks # the input to this model is z-dim, not including the batch dimension. ls_in = Input(shape=(params.latent_dim,), name=\"prop_pred_input\") prop_mid = ls_in # shared weights part for p_i in range(params.prop_pred_depth): prop_mid = Dense( int(params.prop_hidden_dim * params.prop_growth_factor**p_i), activation=params.prop_pred_activation, name=f\"property_predictor_dense{p_i}\", )(prop_mid) if params.prop_pred_dropout > 0: prop_mid = Dropout(params.prop_pred_dropout)(prop_mid) if params.prop_batchnorm: prop_mid = BatchNormalization()(prop_mid) # for regression tasks logit_prop_pred = None reg_prop_pred = None if isinstance(reg_prop_tasks, list) and len(reg_prop_tasks) > 0: reg_prop_pred = Dense( units=len(reg_prop_tasks), activation=\"linear\", # linear for the output, since may be - name=\"reg_property_output\", )(prop_mid) # for logistic tasks if isinstance(logit_prop_tasks, list) and len(logit_prop_tasks) > 0: logit_prop_pred = Dense( units=len(logit_prop_tasks), activation=\"sigmoid\", # may be better softmax name=\"logit_property_output\", )(prop_mid) # both regression and logistic if logit_prop_pred and reg_prop_pred: return Model(ls_in, [reg_prop_pred, logit_prop_pred], name=\"property_predictor\") # regression only scenario elif reg_prop_pred: return Model(ls_in, reg_prop_pred, name=\"property_predictor\") # logit only scenario elif logit_prop_pred: return Model(ls_in, logit_prop_pred, name=\"property_predictor\") raise Exception( \"No model was created. To run property prediction add the tasks' list in the configuration.\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;property_predictor_model"},{"location":"models.py/#chemvae.models.sample_latent_vector","text":"Create variational layers. Adds noise to z. z_mean: from encoder's Dense()(z). z_log_var: forks into z_mean and z_mean_log_var kl_loss_var: ... params: parameter dictionary passed throughout entire model. Source code in chemvae/models.py def sample_latent_vector(z_mean, shared, kl_loss_var: Variable, params: Config): \"\"\"Create variational layers. Adds noise to z. z_mean: from encoder's Dense()(z). z_log_var: forks into z_mean and z_mean_log_var kl_loss_var: ... params: parameter dictionary passed throughout entire model. \"\"\" # why concatenate? z_log_var = Dense(units=params.latent_dim, name=\"z_log_var\")(shared) z_mean_z_log_var_output = Concatenate(name=\"z_mean_z_log_var\")([z_mean, z_log_var]) z_samp = Sampling( kl_loss_var=kl_loss_var, rand_seed=params.RAND_SEED, name=\"z_samp\", )([z_mean, z_log_var]) if params.batchnorm_vae: z_samp = BatchNormalization(axis=-1)(z_samp) return z_samp, z_mean_z_log_var_output","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;sample_latent_vector"},{"location":"mol_callbacks.py/","text":"Callbacks for the training process. Classes EncoderDecoderCheckpoint(encoder_model: Functional, decoder_model: Functional, parameters: Config, prop_pred_model: Functional | None = None, prop_to_monitor='val_x_pred_categorical_accuracy', save_best_only=True, monitor_op=np.greater, monitor_best_init=-np.Inf) Bases: Callback Save Encoder, Decoder and optionally the Property predictor. Save models. If save_best_only=True , uses the accuracy to decide whether to save it or not. encoder_model : the encoder. decoder_model : the decoder. parameters : our program configuration. prop_pred_model : the property predictor. prop_to_monitor : a property that is a valid name in the model. These properties come from the training epoch-logs? Where do we specify which metrics the training tracks/monitors? To which of the models is it referred? save_best_only : whether to compare to previous or save all checkpoints. monitor_op : The operation to use when monitoring the property (e.g. accuracy to be maximized so use np.greater , loss to minimized, so use np.less ) monitor_best_init : starting point for monitor (use -np.Inf for maximization tests, and np.Inf for minimization tests) Source code in chemvae/mol_callbacks.py def __init__( self, encoder_model: Functional, decoder_model: Functional, parameters: Config, prop_pred_model: Functional | None = None, prop_to_monitor=\"val_x_pred_categorical_accuracy\", save_best_only=True, monitor_op=np.greater, monitor_best_init=-np.Inf, ): \"\"\"Save models. If `save_best_only=True`, uses the accuracy to decide whether to save it or not. - `encoder_model`: the encoder. - `decoder_model`: the decoder. - `parameters`: our program configuration. - `prop_pred_model`: the property predictor. - `prop_to_monitor`: a property that is a valid name in the model. These properties come from the training epoch-logs? Where do we specify which metrics the training tracks/monitors? To which of the models is it referred? - `save_best_only`: whether to compare to previous or save all checkpoints. - `monitor_op`: The operation to use when monitoring the property (e.g. accuracy to be maximized so use `np.greater`, loss to minimized, so use `np.less`) - `monitor_best_init`: starting point for monitor (use `-np.Inf` for maximization tests, and `np.Inf` for minimization tests) \"\"\" super().__init__() self.p = parameters self.save_path = Path(self.p.checkpoint_path) self.encoder = encoder_model self.decoder = decoder_model self.prop_pred_model = prop_pred_model self.save_best_only = save_best_only self.monitor = prop_to_monitor self.monitor_op = monitor_op self.best = monitor_best_init self.verbose = self.p.verbosity Functions WeightAnnealerEpoch(schedule: Schedule, kl_loss_var: Variable, param_kl_loss_weight: float, weight_name: str) Bases: Callback Adjust kl weight. Called on_epoch_begin during training. Configure WeightAnnealerEpoch. schedule : fn taking an epoch's index as input Returns updated kl loss. kl_loss_var : The loss-variable to be updated. param_kl_loss_weight : initial weight, from the parameters file. weight_name : friendly identified for printing. Source code in chemvae/mol_callbacks.py def __init__( self, schedule: Schedule, kl_loss_var: Variable, param_kl_loss_weight: float, weight_name: str, ): \"\"\"Configure WeightAnnealerEpoch. - `schedule`: - fn taking an epoch's index as input - Returns updated kl loss. - `kl_loss_var`: The loss-variable to be updated. - `param_kl_loss_weight`: initial weight, from the parameters file. - `weight_name`: friendly identified for printing. \"\"\" super().__init__() self.schedule = schedule # takes epoch, returns new weight self.weight_var = kl_loss_var self.param_kl_loss_weight = param_kl_loss_weight self.weight_name = weight_name Functions on_epoch_begin(epoch: int, logs=None) Make new numeric weight (float) for the VAE. Source code in chemvae/mol_callbacks.py def on_epoch_begin(self, epoch: int, logs=None): \"\"\"Make new numeric weight (float) for the VAE.\"\"\" logs = logs or {} new_weight = self.schedule(epoch) # new_value = new_weight * self.param_kl_loss_weight self.weight_var.assign(new_value) print(\"Current {} annealer weight is {}\".format(self.weight_name, new_value)) Functions sigmoid_schedule(time_step: float | int, start: float, slope: float = 1.0) Make new kl weight. This function is passed into partial . - time_step : epoch/index. - start : param from config (fixed val) - slope : param from config (fixed val) convert back from numpy float to float The output should increase towards 1.0 Source code in chemvae/mol_callbacks.py def sigmoid_schedule(time_step: float | int, start: float, slope: float = 1.0): \"\"\"Make new kl weight. This function is passed into `partial`. - `time_step`: epoch/index. - `start`: param from config (fixed val) - `slope`: param from config (fixed val) convert back from numpy float to float The output should increase towards 1.0 \"\"\" return float(1.0 / (1.0 + np.exp(slope * (start - time_step))))","title":"Mol callbacks.py"},{"location":"mol_callbacks.py/#chemvae.mol_callbacks-classes","text":"","title":"Classes"},{"location":"mol_callbacks.py/#chemvae.mol_callbacks.EncoderDecoderCheckpoint","text":"Bases: Callback Save Encoder, Decoder and optionally the Property predictor. Save models. If save_best_only=True , uses the accuracy to decide whether to save it or not. encoder_model : the encoder. decoder_model : the decoder. parameters : our program configuration. prop_pred_model : the property predictor. prop_to_monitor : a property that is a valid name in the model. These properties come from the training epoch-logs? Where do we specify which metrics the training tracks/monitors? To which of the models is it referred? save_best_only : whether to compare to previous or save all checkpoints. monitor_op : The operation to use when monitoring the property (e.g. accuracy to be maximized so use np.greater , loss to minimized, so use np.less ) monitor_best_init : starting point for monitor (use -np.Inf for maximization tests, and np.Inf for minimization tests) Source code in chemvae/mol_callbacks.py def __init__( self, encoder_model: Functional, decoder_model: Functional, parameters: Config, prop_pred_model: Functional | None = None, prop_to_monitor=\"val_x_pred_categorical_accuracy\", save_best_only=True, monitor_op=np.greater, monitor_best_init=-np.Inf, ): \"\"\"Save models. If `save_best_only=True`, uses the accuracy to decide whether to save it or not. - `encoder_model`: the encoder. - `decoder_model`: the decoder. - `parameters`: our program configuration. - `prop_pred_model`: the property predictor. - `prop_to_monitor`: a property that is a valid name in the model. These properties come from the training epoch-logs? Where do we specify which metrics the training tracks/monitors? To which of the models is it referred? - `save_best_only`: whether to compare to previous or save all checkpoints. - `monitor_op`: The operation to use when monitoring the property (e.g. accuracy to be maximized so use `np.greater`, loss to minimized, so use `np.less`) - `monitor_best_init`: starting point for monitor (use `-np.Inf` for maximization tests, and `np.Inf` for minimization tests) \"\"\" super().__init__() self.p = parameters self.save_path = Path(self.p.checkpoint_path) self.encoder = encoder_model self.decoder = decoder_model self.prop_pred_model = prop_pred_model self.save_best_only = save_best_only self.monitor = prop_to_monitor self.monitor_op = monitor_op self.best = monitor_best_init self.verbose = self.p.verbosity","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;EncoderDecoderCheckpoint"},{"location":"mol_callbacks.py/#chemvae.mol_callbacks.EncoderDecoderCheckpoint-functions","text":"","title":"Functions"},{"location":"mol_callbacks.py/#chemvae.mol_callbacks.WeightAnnealerEpoch","text":"Bases: Callback Adjust kl weight. Called on_epoch_begin during training. Configure WeightAnnealerEpoch. schedule : fn taking an epoch's index as input Returns updated kl loss. kl_loss_var : The loss-variable to be updated. param_kl_loss_weight : initial weight, from the parameters file. weight_name : friendly identified for printing. Source code in chemvae/mol_callbacks.py def __init__( self, schedule: Schedule, kl_loss_var: Variable, param_kl_loss_weight: float, weight_name: str, ): \"\"\"Configure WeightAnnealerEpoch. - `schedule`: - fn taking an epoch's index as input - Returns updated kl loss. - `kl_loss_var`: The loss-variable to be updated. - `param_kl_loss_weight`: initial weight, from the parameters file. - `weight_name`: friendly identified for printing. \"\"\" super().__init__() self.schedule = schedule # takes epoch, returns new weight self.weight_var = kl_loss_var self.param_kl_loss_weight = param_kl_loss_weight self.weight_name = weight_name","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;WeightAnnealerEpoch"},{"location":"mol_callbacks.py/#chemvae.mol_callbacks.WeightAnnealerEpoch-functions","text":"","title":"Functions"},{"location":"mol_callbacks.py/#chemvae.mol_callbacks.WeightAnnealerEpoch.on_epoch_begin","text":"Make new numeric weight (float) for the VAE. Source code in chemvae/mol_callbacks.py def on_epoch_begin(self, epoch: int, logs=None): \"\"\"Make new numeric weight (float) for the VAE.\"\"\" logs = logs or {} new_weight = self.schedule(epoch) # new_value = new_weight * self.param_kl_loss_weight self.weight_var.assign(new_value) print(\"Current {} annealer weight is {}\".format(self.weight_name, new_value))","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;on_epoch_begin"},{"location":"mol_callbacks.py/#chemvae.mol_callbacks-functions","text":"","title":"Functions"},{"location":"mol_callbacks.py/#chemvae.mol_callbacks.sigmoid_schedule","text":"Make new kl weight. This function is passed into partial . - time_step : epoch/index. - start : param from config (fixed val) - slope : param from config (fixed val) convert back from numpy float to float The output should increase towards 1.0 Source code in chemvae/mol_callbacks.py def sigmoid_schedule(time_step: float | int, start: float, slope: float = 1.0): \"\"\"Make new kl weight. This function is passed into `partial`. - `time_step`: epoch/index. - `start`: param from config (fixed val) - `slope`: param from config (fixed val) convert back from numpy float to float The output should increase towards 1.0 \"\"\" return float(1.0 / (1.0 + np.exp(slope * (start - time_step))))","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;sigmoid_schedule"},{"location":"mol_utils.py/","text":"Convert and check smiles. Load and write dataframe. Functions canon_smiles(smi: str) -> str Smile to canonical smile. Doesn't rdkit have a canonicalise smile fn? Source code in chemvae/mol_utils.py def canon_smiles(smi: str) -> str: \"\"\"Smile to canonical smile. Doesn't rdkit have a canonicalise smile fn? \"\"\" return Chem.MolToSmiles( Chem.MolFromSmiles(smi), isomericSmiles=True, canonical=True ) fast_verify(s: str) Ring and balanced parenthesis check. Source code in chemvae/mol_utils.py def fast_verify(s: str): \"\"\"Ring and balanced parenthesis check.\"\"\" return matched_ring(s) and balanced_parentheses(s) filter_valid_length(strings: list[str], max_len: int) Return strings with length <= the max length. Source code in chemvae/mol_utils.py def filter_valid_length(strings: list[str], max_len: int): \"\"\"Return strings with length <= the max length.\"\"\" return [s for s in strings if len(s) <= max_len] filter_valid_smiles_return_invalid(strings: list[str], max_len: int) Filter strings with length above max_length. Returns: 1. A list of valid strings (list[str]) \u2013 2. A list of indices of invalid strings. (list[int]) \u2013 Source code in chemvae/mol_utils.py def filter_valid_smiles_return_invalid(strings: list[str], max_len: int): \"\"\"Filter strings with length above max_length. Returns ------- 1. A list of valid strings (list[str]) 2. A list of indices of invalid strings. (list[int]) \"\"\" filter_list: list[int] = [] new_smiles: list[str] = [] for idx, s in enumerate(strings): if len(s) > max_len: filter_list.append(idx) else: new_smiles.append(s) return new_smiles, filter_list good_smiles(smile: str) Verify smile and return either canonical or none. Returns: smile or None. Source code in chemvae/mol_utils.py def good_smiles(smile: str): \"\"\"Verify smile and return either canonical or none. Returns: smile or None. \"\"\" if verify_smiles(smile): # not falsy return canon_smiles(smile) else: return None hot_to_smiles(hot_x: Tensor3D, indices_chars: dict[int, str]) Return: smile string from argmax(hot_smile) Source code in chemvae/mol_utils.py def hot_to_smiles(hot_x: Tensor3D, indices_chars: dict[int, str]): \"\"\"Return: smile string from argmax(hot_smile)\"\"\" smiles: list[str] = [] for x in hot_x: # go over rows (smiles) temp_str = \"\" for j in x: # go over characters, as Probs arrays index = np.argmax(j) # get max index temp_str += indices_chars[index] smiles.append(temp_str) return smiles load_smiles(smi_file, max_len: int | None = None, return_filtered=False) Get smiles under max length from file. Opt: Pad to max_len, if max_len is specified Opt: get the indices of bad smiles Returns: nice_smiles_list [, bad_smiles_indices_list ] Source code in chemvae/mol_utils.py def load_smiles(smi_file, max_len: int | None = None, return_filtered=False): \"\"\"1. Get smiles under max length from file. 2. Opt: Pad to max_len, if max_len is specified 3. Opt: get the indices of bad smiles Returns: nice_smiles_list [, bad_smiles_indices_list ] \"\"\" if smi_file[-4:] == \".pkl\": # binary file with open(smi_file, \"rb\") as f: smiles = pkl.load(f) # visible outside the scope as well else: # assume file is a text file with open(smi_file, \"r\") as f: smiles = f.readlines() smiles = [i.strip() for i in smiles] if max_len is not None: if return_filtered: smiles, filtrate = filter_valid_smiles_return_invalid(smiles, max_len) if len(filtrate) > 0: print(\"Filtered out {} smiles above max_len\".format(len(filtrate))) return smiles, filtrate else: old_len = len(smiles) smiles = filter_valid_length(smiles, max_len) diff_len = old_len - len(smiles) if diff_len != 0: print(\"Filtered out {} smiles above max_len\".format(diff_len)) return smiles load_smiles_and_data_df(csv_file_path: str, max_len: int, reg_tasks: list[str] | None = None, logit_tasks: list[str] | None = None, normalize_out: str | None = None, dtype: str = 'float64') loads csv as df from path and filters using max_len. Splits into data_df_filtered, smiles_series_filtered smiles_series.tolist() Computes properties of the data_df_filtered. Writes out the statistics for the selected columns (if normalise_out!=None) Arguments: csv_file_path: path to data file max_len: filters long smiles reg_tasks : [colnames] that correspond to regression tasks. logit_tasks : [same] for logit tasks normalize_out: path to write out normalized reg cols (only) csv or None. Source code in chemvae/mol_utils.py def load_smiles_and_data_df( csv_file_path: str, max_len: int, reg_tasks: list[str] | None = None, logit_tasks: list[str] | None = None, normalize_out: str | None = None, dtype: str = \"float64\", ): \"\"\"1. loads csv as df from path and filters using max_len. 2. Splits into data_df_filtered, smiles_series_filtered 3. `smiles_series.tolist()` 4. Computes properties of the data_df_filtered. 5. Writes out the statistics for the selected columns (if normalise_out!=None) Arguments: --------- csv_file_path: path to data file max_len: filters long smiles reg_tasks : [colnames] that correspond to regression tasks. logit_tasks : [same] for logit tasks normalize_out: path to write out normalized reg cols (only) csv or None. \"\"\" if logit_tasks is None: logit_tasks = [] if reg_tasks is None: reg_tasks = [] # filters only on max_len, splits dataframe df, smiles_series = smiles_and_full_df(csv_file_path, max_len) smiles: list[str] = smiles_series.tolist() reg_data_df = df[reg_tasks] # subsets of columns logit_data_df = df[logit_tasks] # subsets of columns # normalise regression data in df if len(reg_tasks) != 0 and normalize_out is not None: df_norm = pd.DataFrame() # stats for each column. df_norm[\"mean\"] = reg_data_df.mean(axis=0) df_norm[\"std\"] = reg_data_df.std(axis=0) reg_data_df = (reg_data_df - df_norm[\"mean\"]) / df_norm[\"std\"] # write out normalised colums csv. df_norm.to_csv(normalize_out) # this will be loaded by the sampler ! # return new arrays if len(logit_tasks) != 0 and len(reg_tasks) != 0: # .values returns the table as an array. # I removed the np.vstack(reg_data_df.values) return ( smiles, reg_data_df.values.astype(dtype), logit_data_df.values.astype(dtype), ) elif len(reg_tasks) != 0: return smiles, reg_data_df.values.astype(dtype), None elif len(logit_tasks) != 0: return smiles, logit_data_df.values.astype(dtype), None else: return smiles, None, None make_charset_list_from_list(smi_list) Create a list of unique characters. Does not hot encode the chars. Source code in chemvae/mol_utils.py def make_charset_list_from_list(smi_list): \"\"\"Create a list of unique characters. Does not hot encode the chars. \"\"\" char_lists = [list(smi) for smi in smi_list] chars = list(set([char for sub_list in char_lists for char in sub_list])) chars.append(\" \") return chars matched_ring(s: str) Check that rings are represented correctly. Example: c1cccccc1 i.e (c1's close the ring.) c: aromatic, C: aliphatic Source code in chemvae/mol_utils.py def matched_ring(s: str): \"\"\"Check that rings are represented correctly. Example: c1cccccc1 i.e (c1's close the ring.) c: aromatic, C: aliphatic \"\"\" return s.count(\"1\") % 2 == 0 and s.count(\"2\") % 2 == 0 pad_smile(string: str, max_len: int, padding: Padding_type = 'right') -> str For \"string length > max\" raise exception. Return: smile padded to the max length Source code in chemvae/mol_utils.py def pad_smile(string: str, max_len: int, padding: Padding_type = \"right\") -> str: \"\"\"For \"string length > max\" raise exception. Return: smile padded to the max length \"\"\" to_add = max_len - len(string) if is_valid_len(string, max_len): if padding == \"right\": return string + \" \" * to_add elif padding == \"left\": return \" \" * to_add elif padding == \"none\": return string else: raise Exception(\"padding must be left|right|none.\") else: raise Exception(\"Smile was too long.\") smiles_and_full_df(csv_file_path: str, max_len: int) Split df, smiles. smiles are <= max_len & and stripped out. Source code in chemvae/mol_utils.py def smiles_and_full_df(csv_file_path: str, max_len: int): \"\"\"Split df, smiles. smiles are <= max_len & and stripped out. \"\"\" # I guess smiles are within the pandas dataframe. df = pd.read_csv(csv_file_path) # strip strings # df.iloc[:, 0] = df.iloc[:, 0].str.strip() smi_col_name = df.columns[0] df[smi_col_name] = df[smi_col_name].str.strip() # select good smiles # df = df[df.iloc[:, 0].str.len() <= max_len] df = df[df[smi_col_name].str.len() <= max_len] smiles_df = df[smi_col_name] return df, smiles_df smiles_to_hot(smiles: list[str], max_len: int, padding: Padding_type, char_indices: dict[str, int], nchars: int) Populate binary tensor (smiles, max_len, nchars). Source code in chemvae/mol_utils.py def smiles_to_hot( smiles: list[str], max_len: int, padding: Padding_type, char_indices: dict[str, int], nchars: int, ): \"\"\"Populate binary tensor (smiles, max_len, nchars).\"\"\" smiles = [ pad_smile(i, max_len, padding) for i in smiles if is_valid_len(i, max_len) ] # nchars=n of components of each hot-enc vector (n of != strings available in dataset.) # nice tensor X = np.zeros((len(smiles), max_len, nchars), dtype=np.float32) # populate it for i, smile in enumerate(smiles): for t, char in enumerate(smile): try: # for example \"c\" has some char index in that dictionary. X[i, t, char_indices[char]] = 1 except KeyError as e: # if the lookup fails print(\"ERROR: Check chars file. Bad SMILES: \", smile, \"char n: \", char) raise e return X smiles_to_hot_filter(smiles: list[str], char_indices: dict[str, int]) Put smiles with \"non-supported\" characters into a list. Source code in chemvae/mol_utils.py def smiles_to_hot_filter(smiles: list[str], char_indices: dict[str, int]): \"\"\"Put smiles with \"non-supported\" characters into a list.\"\"\" filtered_smiles: list[str] = [] for smile in smiles: for char in smile: try: char_indices[char] except KeyError: break else: filtered_smiles.append(smile) return filtered_smiles smiles_to_mol(smiles: str) -> None | Mol Check that it can be converted to Mol. Otherwise catches the error and returns None Returns: None|Mol Source code in chemvae/mol_utils.py def smiles_to_mol(smiles: str) -> None | Mol: \"\"\"Check that it can be converted to Mol. Otherwise catches the error and returns None Returns: None|Mol \"\"\" try: mol = Chem.MolFromSmiles(smiles) return mol except Exception as ex: print(f\"can not convert {smiles}\", ex) pass return None term_hot_to_smiles(hot_x: list[np.ndarray], temperature: float, indices_chars: dict[int, str]) Pass the logits through softmax (more or less.). hot_x = hot encoded smile Return: smile string Source code in chemvae/mol_utils.py def term_hot_to_smiles( hot_x: list[np.ndarray], temperature: float, indices_chars: dict[int, str] ): \"\"\"Pass the logits through softmax (more or less.). hot_x = hot encoded smile Return: smile string \"\"\" temp_string = \"\" for j in hot_x: # j=character as probs vector # get max index after softmax (more or less) index = thermal_argmax(j, temperature) # map to string. temp_string += indices_chars[index] return temp_string thermal_argmax(prob_arr: np.ndarray, temperature: float) Add a little bit of probabilistic behaviour to the chosen string. Source code in chemvae/mol_utils.py def thermal_argmax(prob_arr: np.ndarray, temperature: float): \"\"\"Add a little bit of probabilistic behaviour to the chosen string.\"\"\" prob_arr = np.log(prob_arr) / temperature prob_arr = np.exp(prob_arr) / np.sum(np.exp(prob_arr)) print(prob_arr) if np.greater_equal(prob_arr.sum(), 1.0000000001): logging.warn( \"Probabilities to sample add to more than 1, {}\".format(prob_arr.sum()) ) prob_arr = prob_arr / (prob_arr.sum() + 0.0000000001) if np.greater_equal(prob_arr.sum(), 1.0000000001): logging.warn(\"Probabilities to sample still add to more than 1\") # imagine a dice where each face has a P, this is what they do here. # it returns a 1 in the selected component of the array of same length. return np.argmax(np.random.multinomial(1, prob_arr, 1)) verify_smiles(smile: str) Check that smile is not falsy. Source code in chemvae/mol_utils.py def verify_smiles(smile: str): \"\"\"Check that smile is not falsy.\"\"\" return ( (smile != \"\") and pd.notnull(smile) and (Chem.MolFromSmiles(smile) is not None) )","title":"Mol utils.py"},{"location":"mol_utils.py/#chemvae.mol_utils-functions","text":"","title":"Functions"},{"location":"mol_utils.py/#chemvae.mol_utils.canon_smiles","text":"Smile to canonical smile. Doesn't rdkit have a canonicalise smile fn? Source code in chemvae/mol_utils.py def canon_smiles(smi: str) -> str: \"\"\"Smile to canonical smile. Doesn't rdkit have a canonicalise smile fn? \"\"\" return Chem.MolToSmiles( Chem.MolFromSmiles(smi), isomericSmiles=True, canonical=True )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;canon_smiles"},{"location":"mol_utils.py/#chemvae.mol_utils.fast_verify","text":"Ring and balanced parenthesis check. Source code in chemvae/mol_utils.py def fast_verify(s: str): \"\"\"Ring and balanced parenthesis check.\"\"\" return matched_ring(s) and balanced_parentheses(s)","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;fast_verify"},{"location":"mol_utils.py/#chemvae.mol_utils.filter_valid_length","text":"Return strings with length <= the max length. Source code in chemvae/mol_utils.py def filter_valid_length(strings: list[str], max_len: int): \"\"\"Return strings with length <= the max length.\"\"\" return [s for s in strings if len(s) <= max_len]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;filter_valid_length"},{"location":"mol_utils.py/#chemvae.mol_utils.filter_valid_smiles_return_invalid","text":"Filter strings with length above max_length. Returns: 1. A list of valid strings (list[str]) \u2013 2. A list of indices of invalid strings. (list[int]) \u2013 Source code in chemvae/mol_utils.py def filter_valid_smiles_return_invalid(strings: list[str], max_len: int): \"\"\"Filter strings with length above max_length. Returns ------- 1. A list of valid strings (list[str]) 2. A list of indices of invalid strings. (list[int]) \"\"\" filter_list: list[int] = [] new_smiles: list[str] = [] for idx, s in enumerate(strings): if len(s) > max_len: filter_list.append(idx) else: new_smiles.append(s) return new_smiles, filter_list","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;filter_valid_smiles_return_invalid"},{"location":"mol_utils.py/#chemvae.mol_utils.good_smiles","text":"Verify smile and return either canonical or none. Returns: smile or None. Source code in chemvae/mol_utils.py def good_smiles(smile: str): \"\"\"Verify smile and return either canonical or none. Returns: smile or None. \"\"\" if verify_smiles(smile): # not falsy return canon_smiles(smile) else: return None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;good_smiles"},{"location":"mol_utils.py/#chemvae.mol_utils.hot_to_smiles","text":"Return: smile string from argmax(hot_smile) Source code in chemvae/mol_utils.py def hot_to_smiles(hot_x: Tensor3D, indices_chars: dict[int, str]): \"\"\"Return: smile string from argmax(hot_smile)\"\"\" smiles: list[str] = [] for x in hot_x: # go over rows (smiles) temp_str = \"\" for j in x: # go over characters, as Probs arrays index = np.argmax(j) # get max index temp_str += indices_chars[index] smiles.append(temp_str) return smiles","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;hot_to_smiles"},{"location":"mol_utils.py/#chemvae.mol_utils.load_smiles","text":"Get smiles under max length from file. Opt: Pad to max_len, if max_len is specified Opt: get the indices of bad smiles Returns: nice_smiles_list [, bad_smiles_indices_list ] Source code in chemvae/mol_utils.py def load_smiles(smi_file, max_len: int | None = None, return_filtered=False): \"\"\"1. Get smiles under max length from file. 2. Opt: Pad to max_len, if max_len is specified 3. Opt: get the indices of bad smiles Returns: nice_smiles_list [, bad_smiles_indices_list ] \"\"\" if smi_file[-4:] == \".pkl\": # binary file with open(smi_file, \"rb\") as f: smiles = pkl.load(f) # visible outside the scope as well else: # assume file is a text file with open(smi_file, \"r\") as f: smiles = f.readlines() smiles = [i.strip() for i in smiles] if max_len is not None: if return_filtered: smiles, filtrate = filter_valid_smiles_return_invalid(smiles, max_len) if len(filtrate) > 0: print(\"Filtered out {} smiles above max_len\".format(len(filtrate))) return smiles, filtrate else: old_len = len(smiles) smiles = filter_valid_length(smiles, max_len) diff_len = old_len - len(smiles) if diff_len != 0: print(\"Filtered out {} smiles above max_len\".format(diff_len)) return smiles","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_smiles"},{"location":"mol_utils.py/#chemvae.mol_utils.load_smiles_and_data_df","text":"loads csv as df from path and filters using max_len. Splits into data_df_filtered, smiles_series_filtered smiles_series.tolist() Computes properties of the data_df_filtered. Writes out the statistics for the selected columns (if normalise_out!=None) Arguments: csv_file_path: path to data file max_len: filters long smiles reg_tasks : [colnames] that correspond to regression tasks. logit_tasks : [same] for logit tasks normalize_out: path to write out normalized reg cols (only) csv or None. Source code in chemvae/mol_utils.py def load_smiles_and_data_df( csv_file_path: str, max_len: int, reg_tasks: list[str] | None = None, logit_tasks: list[str] | None = None, normalize_out: str | None = None, dtype: str = \"float64\", ): \"\"\"1. loads csv as df from path and filters using max_len. 2. Splits into data_df_filtered, smiles_series_filtered 3. `smiles_series.tolist()` 4. Computes properties of the data_df_filtered. 5. Writes out the statistics for the selected columns (if normalise_out!=None) Arguments: --------- csv_file_path: path to data file max_len: filters long smiles reg_tasks : [colnames] that correspond to regression tasks. logit_tasks : [same] for logit tasks normalize_out: path to write out normalized reg cols (only) csv or None. \"\"\" if logit_tasks is None: logit_tasks = [] if reg_tasks is None: reg_tasks = [] # filters only on max_len, splits dataframe df, smiles_series = smiles_and_full_df(csv_file_path, max_len) smiles: list[str] = smiles_series.tolist() reg_data_df = df[reg_tasks] # subsets of columns logit_data_df = df[logit_tasks] # subsets of columns # normalise regression data in df if len(reg_tasks) != 0 and normalize_out is not None: df_norm = pd.DataFrame() # stats for each column. df_norm[\"mean\"] = reg_data_df.mean(axis=0) df_norm[\"std\"] = reg_data_df.std(axis=0) reg_data_df = (reg_data_df - df_norm[\"mean\"]) / df_norm[\"std\"] # write out normalised colums csv. df_norm.to_csv(normalize_out) # this will be loaded by the sampler ! # return new arrays if len(logit_tasks) != 0 and len(reg_tasks) != 0: # .values returns the table as an array. # I removed the np.vstack(reg_data_df.values) return ( smiles, reg_data_df.values.astype(dtype), logit_data_df.values.astype(dtype), ) elif len(reg_tasks) != 0: return smiles, reg_data_df.values.astype(dtype), None elif len(logit_tasks) != 0: return smiles, logit_data_df.values.astype(dtype), None else: return smiles, None, None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_smiles_and_data_df"},{"location":"mol_utils.py/#chemvae.mol_utils.make_charset_list_from_list","text":"Create a list of unique characters. Does not hot encode the chars. Source code in chemvae/mol_utils.py def make_charset_list_from_list(smi_list): \"\"\"Create a list of unique characters. Does not hot encode the chars. \"\"\" char_lists = [list(smi) for smi in smi_list] chars = list(set([char for sub_list in char_lists for char in sub_list])) chars.append(\" \") return chars","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;make_charset_list_from_list"},{"location":"mol_utils.py/#chemvae.mol_utils.matched_ring","text":"Check that rings are represented correctly. Example: c1cccccc1 i.e (c1's close the ring.) c: aromatic, C: aliphatic Source code in chemvae/mol_utils.py def matched_ring(s: str): \"\"\"Check that rings are represented correctly. Example: c1cccccc1 i.e (c1's close the ring.) c: aromatic, C: aliphatic \"\"\" return s.count(\"1\") % 2 == 0 and s.count(\"2\") % 2 == 0","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;matched_ring"},{"location":"mol_utils.py/#chemvae.mol_utils.pad_smile","text":"For \"string length > max\" raise exception. Return: smile padded to the max length Source code in chemvae/mol_utils.py def pad_smile(string: str, max_len: int, padding: Padding_type = \"right\") -> str: \"\"\"For \"string length > max\" raise exception. Return: smile padded to the max length \"\"\" to_add = max_len - len(string) if is_valid_len(string, max_len): if padding == \"right\": return string + \" \" * to_add elif padding == \"left\": return \" \" * to_add elif padding == \"none\": return string else: raise Exception(\"padding must be left|right|none.\") else: raise Exception(\"Smile was too long.\")","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;pad_smile"},{"location":"mol_utils.py/#chemvae.mol_utils.smiles_and_full_df","text":"Split df, smiles. smiles are <= max_len & and stripped out. Source code in chemvae/mol_utils.py def smiles_and_full_df(csv_file_path: str, max_len: int): \"\"\"Split df, smiles. smiles are <= max_len & and stripped out. \"\"\" # I guess smiles are within the pandas dataframe. df = pd.read_csv(csv_file_path) # strip strings # df.iloc[:, 0] = df.iloc[:, 0].str.strip() smi_col_name = df.columns[0] df[smi_col_name] = df[smi_col_name].str.strip() # select good smiles # df = df[df.iloc[:, 0].str.len() <= max_len] df = df[df[smi_col_name].str.len() <= max_len] smiles_df = df[smi_col_name] return df, smiles_df","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;smiles_and_full_df"},{"location":"mol_utils.py/#chemvae.mol_utils.smiles_to_hot","text":"Populate binary tensor (smiles, max_len, nchars). Source code in chemvae/mol_utils.py def smiles_to_hot( smiles: list[str], max_len: int, padding: Padding_type, char_indices: dict[str, int], nchars: int, ): \"\"\"Populate binary tensor (smiles, max_len, nchars).\"\"\" smiles = [ pad_smile(i, max_len, padding) for i in smiles if is_valid_len(i, max_len) ] # nchars=n of components of each hot-enc vector (n of != strings available in dataset.) # nice tensor X = np.zeros((len(smiles), max_len, nchars), dtype=np.float32) # populate it for i, smile in enumerate(smiles): for t, char in enumerate(smile): try: # for example \"c\" has some char index in that dictionary. X[i, t, char_indices[char]] = 1 except KeyError as e: # if the lookup fails print(\"ERROR: Check chars file. Bad SMILES: \", smile, \"char n: \", char) raise e return X","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;smiles_to_hot"},{"location":"mol_utils.py/#chemvae.mol_utils.smiles_to_hot_filter","text":"Put smiles with \"non-supported\" characters into a list. Source code in chemvae/mol_utils.py def smiles_to_hot_filter(smiles: list[str], char_indices: dict[str, int]): \"\"\"Put smiles with \"non-supported\" characters into a list.\"\"\" filtered_smiles: list[str] = [] for smile in smiles: for char in smile: try: char_indices[char] except KeyError: break else: filtered_smiles.append(smile) return filtered_smiles","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;smiles_to_hot_filter"},{"location":"mol_utils.py/#chemvae.mol_utils.smiles_to_mol","text":"Check that it can be converted to Mol. Otherwise catches the error and returns None Returns: None|Mol Source code in chemvae/mol_utils.py def smiles_to_mol(smiles: str) -> None | Mol: \"\"\"Check that it can be converted to Mol. Otherwise catches the error and returns None Returns: None|Mol \"\"\" try: mol = Chem.MolFromSmiles(smiles) return mol except Exception as ex: print(f\"can not convert {smiles}\", ex) pass return None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;smiles_to_mol"},{"location":"mol_utils.py/#chemvae.mol_utils.term_hot_to_smiles","text":"Pass the logits through softmax (more or less.). hot_x = hot encoded smile Return: smile string Source code in chemvae/mol_utils.py def term_hot_to_smiles( hot_x: list[np.ndarray], temperature: float, indices_chars: dict[int, str] ): \"\"\"Pass the logits through softmax (more or less.). hot_x = hot encoded smile Return: smile string \"\"\" temp_string = \"\" for j in hot_x: # j=character as probs vector # get max index after softmax (more or less) index = thermal_argmax(j, temperature) # map to string. temp_string += indices_chars[index] return temp_string","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;term_hot_to_smiles"},{"location":"mol_utils.py/#chemvae.mol_utils.thermal_argmax","text":"Add a little bit of probabilistic behaviour to the chosen string. Source code in chemvae/mol_utils.py def thermal_argmax(prob_arr: np.ndarray, temperature: float): \"\"\"Add a little bit of probabilistic behaviour to the chosen string.\"\"\" prob_arr = np.log(prob_arr) / temperature prob_arr = np.exp(prob_arr) / np.sum(np.exp(prob_arr)) print(prob_arr) if np.greater_equal(prob_arr.sum(), 1.0000000001): logging.warn( \"Probabilities to sample add to more than 1, {}\".format(prob_arr.sum()) ) prob_arr = prob_arr / (prob_arr.sum() + 0.0000000001) if np.greater_equal(prob_arr.sum(), 1.0000000001): logging.warn(\"Probabilities to sample still add to more than 1\") # imagine a dice where each face has a P, this is what they do here. # it returns a 1 in the selected component of the array of same length. return np.argmax(np.random.multinomial(1, prob_arr, 1))","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;thermal_argmax"},{"location":"mol_utils.py/#chemvae.mol_utils.verify_smiles","text":"Check that smile is not falsy. Source code in chemvae/mol_utils.py def verify_smiles(smile: str): \"\"\"Check that smile is not falsy.\"\"\" return ( (smile != \"\") and pd.notnull(smile) and (Chem.MolFromSmiles(smile) is not None) )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;verify_smiles"},{"location":"other_callbacks.py/","text":"","title":"Other callbacks.py"},{"location":"train_vae.py/","text":"Train the variational autoencoder. Classes NameLayer(name: str) Bases: Layer Name layer between models. This doesn't have any computational meaning. It's like a <div> . :name: the name of the division layer. Name the layer. Source code in chemvae/train_vae.py def __init__(self, name: str): \"\"\"Name the layer.\"\"\" super().__init__(name=name) Functions call(x) Return itself. Source code in chemvae/train_vae.py def call(self, x): \"\"\"Return itself.\"\"\" return x Functions kl_loss(truth_dummy, x_mean_log_var_output) Loss function. x_mean_log_var_output: contains both. Source code in chemvae/train_vae.py def kl_loss(truth_dummy, x_mean_log_var_output): \"\"\"Loss function. x_mean_log_var_output: contains both.\"\"\" x_mean, x_log_var = ops.split(x_mean_log_var_output, 2, axis=1) loss = -0.5 * ops.mean( 1 + x_log_var - ops.square(x_mean) - ops.exp(x_log_var), axis=-1 ) return loss # kl_loss load_models(config: Config, do_prop_pred: bool = False) Load and create each model. config: configuration for networks and program. do_prop_pred: whether to use the property predictor and csv data file. Source code in chemvae/train_vae.py def load_models(config: Config, do_prop_pred: bool = False): \"\"\"Load and create each model. config: configuration for networks and program. do_prop_pred: whether to use the property predictor and csv data file. \"\"\" kl_loss_var = Variable(config.kl_loss_weight, dtype=np.float32) if config.reload_model: encoder = load_encoder(config) decoder = load_decoder(config) else: # new models encoder = encoder_model(config) decoder = decoder_model(config) x_in = encoder.inputs[0] # flat list of symbolic inputs (1 here.) z_mean, z_log_var = encoder(x_in) # this is what makes it variational. z_samp, z_mean_z_log_var_output = sample_latent_vector( z_mean, z_log_var, kl_loss_var, config ) # Decoder x_out = decoder(z_samp) x_out = NameLayer(name=\"x_pred\")(x_out) # decoder output model_outputs = [x_out, z_mean_z_log_var_output] AE_only_model = Model(x_in, model_outputs, name=\"AE_ONLY\") # extends the output if do_prop_pred: if config.reload_model: property_predictor = load_property_predictor(config) else: property_predictor = property_predictor_model(config) if ( isinstance(config.reg_prop_tasks, list) and (len(config.reg_prop_tasks) > 0) and isinstance(config.logit_prop_tasks, list) and (len(config.logit_prop_tasks) > 0) ): reg_prop_pred, logit_prop_pred = property_predictor(z_mean) reg_prop_pred = NameLayer(name=\"reg_prop_pred\")(reg_prop_pred) logit_prop_pred = NameLayer(name=\"logit_prop_pred\")(logit_prop_pred) model_outputs.extend([reg_prop_pred, logit_prop_pred]) # regression only scenario elif isinstance(config.reg_prop_tasks, list) and ( len(config.reg_prop_tasks) > 0 ): reg_prop_pred = property_predictor(z_mean) reg_prop_pred = NameLayer(name=\"reg_prop_pred\")(reg_prop_pred) model_outputs.append(reg_prop_pred) # logit only scenario elif ( isinstance(config.logit_prop_tasks, list) and len(config.logit_prop_tasks) > 0 ): logit_prop_pred = property_predictor(z_mean) logit_prop_pred = NameLayer(name=\"logit_prop_pred\")(logit_prop_pred) model_outputs.append(logit_prop_pred) else: raise ValueError( \"no logit tasks or regression tasks specified for property prediction\" ) # making the models: AE_PP_model = Model(x_in, model_outputs) return ( AE_only_model, AE_PP_model, encoder, decoder, property_predictor, kl_loss_var, ) else: return AE_only_model, encoder, decoder, kl_loss_var","title":"Train vae.py"},{"location":"train_vae.py/#chemvae.train_vae-classes","text":"","title":"Classes"},{"location":"train_vae.py/#chemvae.train_vae.NameLayer","text":"Bases: Layer Name layer between models. This doesn't have any computational meaning. It's like a <div> . :name: the name of the division layer. Name the layer. Source code in chemvae/train_vae.py def __init__(self, name: str): \"\"\"Name the layer.\"\"\" super().__init__(name=name)","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;NameLayer"},{"location":"train_vae.py/#chemvae.train_vae.NameLayer-functions","text":"","title":"Functions"},{"location":"train_vae.py/#chemvae.train_vae.NameLayer.call","text":"Return itself. Source code in chemvae/train_vae.py def call(self, x): \"\"\"Return itself.\"\"\" return x","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;call"},{"location":"train_vae.py/#chemvae.train_vae-functions","text":"","title":"Functions"},{"location":"train_vae.py/#chemvae.train_vae.kl_loss","text":"Loss function. x_mean_log_var_output: contains both. Source code in chemvae/train_vae.py def kl_loss(truth_dummy, x_mean_log_var_output): \"\"\"Loss function. x_mean_log_var_output: contains both.\"\"\" x_mean, x_log_var = ops.split(x_mean_log_var_output, 2, axis=1) loss = -0.5 * ops.mean( 1 + x_log_var - ops.square(x_mean) - ops.exp(x_log_var), axis=-1 ) return loss # kl_loss","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;kl_loss"},{"location":"train_vae.py/#chemvae.train_vae.load_models","text":"Load and create each model. config: configuration for networks and program. do_prop_pred: whether to use the property predictor and csv data file. Source code in chemvae/train_vae.py def load_models(config: Config, do_prop_pred: bool = False): \"\"\"Load and create each model. config: configuration for networks and program. do_prop_pred: whether to use the property predictor and csv data file. \"\"\" kl_loss_var = Variable(config.kl_loss_weight, dtype=np.float32) if config.reload_model: encoder = load_encoder(config) decoder = load_decoder(config) else: # new models encoder = encoder_model(config) decoder = decoder_model(config) x_in = encoder.inputs[0] # flat list of symbolic inputs (1 here.) z_mean, z_log_var = encoder(x_in) # this is what makes it variational. z_samp, z_mean_z_log_var_output = sample_latent_vector( z_mean, z_log_var, kl_loss_var, config ) # Decoder x_out = decoder(z_samp) x_out = NameLayer(name=\"x_pred\")(x_out) # decoder output model_outputs = [x_out, z_mean_z_log_var_output] AE_only_model = Model(x_in, model_outputs, name=\"AE_ONLY\") # extends the output if do_prop_pred: if config.reload_model: property_predictor = load_property_predictor(config) else: property_predictor = property_predictor_model(config) if ( isinstance(config.reg_prop_tasks, list) and (len(config.reg_prop_tasks) > 0) and isinstance(config.logit_prop_tasks, list) and (len(config.logit_prop_tasks) > 0) ): reg_prop_pred, logit_prop_pred = property_predictor(z_mean) reg_prop_pred = NameLayer(name=\"reg_prop_pred\")(reg_prop_pred) logit_prop_pred = NameLayer(name=\"logit_prop_pred\")(logit_prop_pred) model_outputs.extend([reg_prop_pred, logit_prop_pred]) # regression only scenario elif isinstance(config.reg_prop_tasks, list) and ( len(config.reg_prop_tasks) > 0 ): reg_prop_pred = property_predictor(z_mean) reg_prop_pred = NameLayer(name=\"reg_prop_pred\")(reg_prop_pred) model_outputs.append(reg_prop_pred) # logit only scenario elif ( isinstance(config.logit_prop_tasks, list) and len(config.logit_prop_tasks) > 0 ): logit_prop_pred = property_predictor(z_mean) logit_prop_pred = NameLayer(name=\"logit_prop_pred\")(logit_prop_pred) model_outputs.append(logit_prop_pred) else: raise ValueError( \"no logit tasks or regression tasks specified for property prediction\" ) # making the models: AE_PP_model = Model(x_in, model_outputs) return ( AE_only_model, AE_PP_model, encoder, decoder, property_predictor, kl_loss_var, ) else: return AE_only_model, encoder, decoder, kl_loss_var","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;load_models"},{"location":"vae_utils.py/","text":"Utilities. Classes VAEUtils() Bases: object Grabs parameters, weights, chars, creates chars <=> indices dics. encoder_file: .h5 weights file, decoder_file: .h5 weights file, Set params. Source code in chemvae/vae_utils.py def __init__( self, ): \"\"\"Set params.\"\"\" self.params = config with open(self.params.char_file) as f: chars: list[str] = yaml.safe_load(f) # json self.chars = chars self.params.NCHARS = len(self.chars) # length of charset # char <=> index self.char_indices: dict[str, int] = dict( (c, i) for i, c in enumerate(self.chars) ) self.indices_char: dict[int, str] = dict( (i, c) for i, c in enumerate(self.chars) ) # encoder, decoder, predictor self.enc = load_encoder(self.params) # model self.dec = load_decoder(self.params) # model self.encode, self.decode = self.enc_dec_functions() if self.params.do_prop_pred: self.property_predictor = load_property_predictor(self.params) self.data = None # set from file # Load path/name.csv without normalization as dataframe df, smiles_df = mu.smiles_and_full_df( self.params.data_file, self.params.MAX_LEN ) self.smiles: list[str] = smiles_df.tolist() if df.shape[1] > 1: # set everything but the smiles self.data = pd.DataFrame(df.iloc[:, 1:]) self.estimate_estandarization() return Functions chunks(indices: list[int], n: int) staticmethod Yield successive n-sized chunks from l. indices: list of indices n: number of elements, for a slice of n elements. Source code in chemvae/vae_utils.py @staticmethod def chunks(indices: list[int], n: int): \"\"\"Yield successive n-sized chunks from l. indices: list of indices n: number of elements, for a slice of n elements. \"\"\" for i in range(0, len(indices), n): yield indices[i : i + n] enc_dec_functions(standardized=True) Defines \"decode\" fn wrt experiment parameters. do_tgru being one of them. standardized: if z for .decode(z) is standardized. It also stands for \"standardize\" for the encode(z) function. Source code in chemvae/vae_utils.py def enc_dec_functions(self, standardized=True): \"\"\"Defines \"decode\" fn wrt experiment parameters. do_tgru being one of them. standardized: if z for .decode(z) is standardized. It also stands for \"standardize\" for the encode(z) function. \"\"\" print(\"Using standarized functions? {}\".format(standardized)) def decode(z, standardize=standardized): fake_shape = (z.shape[0], self.params.MAX_LEN, self.params.NCHARS) # no idea what this is for. fake_in = np.zeros(fake_shape) if standardize: return self.dec.predict([self.unstandardize_z(z), fake_in]) else: return self.dec.predict([z, fake_in]) def encode(X: np.ndarray, standardize=standardized): \"\"\"Encodes and optionally standardizes the resulting z. X: hot encoded matrix of smiles. standardize: whether to standardise the resulting z. \"\"\" z = self.enc.predict(X)[0] if standardize: # in this case it's not predicting? return self.standardize_z(z) else: return z return encode, decode estimate_estandarization() After the df was split into self.smiles and self.data (un-normalised) This fn accesses self.data Source code in chemvae/vae_utils.py def estimate_estandarization(self): \"\"\"After the df was split into self.smiles and self.data (un-normalised) This fn accesses self.data \"\"\" print(\"Standarization: estimating mu and std values ...\", end=\"\") # random smiles smiles = self.random_molecules(n=50000) batch = 2500 # hidden dim is just the latent vector dimension, a hyperparameter. Z = np.zeros((len(smiles), self.params.latent_dim)) # (50000,len(z)) # [[0-2500], [2500-5000],...] iterator. for chunk in self.chunks(list(range(len(smiles))), batch): # smiles from indices sub_smiles = [smiles[i] for i in chunk] one_hot = self.smiles_to_hot(sub_smiles) # (n_smi,max_len,nchars) # each row z for smile Z[chunk, :] = self.encode(one_hot, False) # (50000, len(z)) # mean for each dimension self.mu = np.mean(Z, axis=0) # [1st col avg, 2nd col avg,...] # std for each dimension self.std = np.std(Z, axis=0) self.Z = self.standardize_z(Z) print(\"done!\") return hot_to_smiles(hot_x: Tensor3D, strip=False) From 3D tensor of Probabilities for each character & smile to smiles flat list. Source code in chemvae/vae_utils.py def hot_to_smiles(self, hot_x: Tensor3D, strip=False): \"\"\"From 3D tensor of Probabilities for each character & smile to smiles flat list. \"\"\" smiles = mu.hot_to_smiles(hot_x, self.indices_char) if strip: # postprocessing smiles = [s.strip() for s in smiles] return smiles ls_sampler_w_prop(size=None, batch=2500, return_smiles=False) Returns encoded (predictions) smiles Z and corresponding data for them. Source code in chemvae/vae_utils.py def ls_sampler_w_prop(self, size=None, batch=2500, return_smiles=False): \"\"\"Returns encoded (predictions) smiles Z and corresponding data for them.\"\"\" if self.data is None: print(\"use this sampler only for external property files\") return cols = [] reg_prop_tasks = self.params.reg_prop_tasks logit_prop_tasks = self.params.logit_prop_tasks if isinstance(reg_prop_tasks, list): cols += reg_prop_tasks if isinstance(logit_prop_tasks, list): cols += logit_prop_tasks # data is the table without smiles column # note that it must be in the same order than smiles. idxs = ( self.random_idxs(size) if isinstance(size, \"int\") else list(range(len(self.smiles))) ) # grab random smiles smiles = [self.smiles[idx] for idx in idxs] # grab corresponding data data = [self.data.iloc[idx] for idx in idxs] # hidden dimensions for all input smiles Z = np.zeros((len(smiles), self.params.latent_dim)) # this avoids having more than 1 cube in memory at a time. for chunk in self.chunks(list(range(len(smiles))), batch): sub_smiles = [smiles[i] for i in chunk] # basically a small_X (3D) one_hot = self.smiles_to_hot(sub_smiles) # will slowly populate the predictions. Z[chunk, :] = self.encode(one_hot) if return_smiles: return Z, data, smiles return Z, data perturb_z(z: np.ndarray, noise_norm: float, constant_norm=False) Tiny deltas to the normalised z but z here is 25 repetitions of z for a string. Source code in chemvae/vae_utils.py def perturb_z(self, z: np.ndarray, noise_norm: float, constant_norm=False): \"\"\"Tiny deltas to the normalised z but z here is 25 repetitions of z for a string. \"\"\" if noise_norm > 0.0: # n between 0 and 1 noise_vec = np.random.normal(0, 1, size=z.shape) # z (50000, 196) # F norm `sqrt(sum of the squared of all values)` # very small. noise_vec = noise_vec / np.linalg.norm(noise_vec) if constant_norm: return z + (noise_norm * noise_vec) else: # each smile a different noise scaling factor noise_amp = np.random.uniform(0, noise_norm, size=(z.shape[0], 1)) return z + (noise_amp * noise_vec) else: return z predict_prop_Z(z: np.ndarray, standardized=True) Predictors from z (encoded vector) Source code in chemvae/vae_utils.py def predict_prop_Z(self, z: np.ndarray, standardized=True): \"\"\"Predictors from z (encoded vector)\"\"\" if standardized: z = self.unstandardize_z(z) # both regression and logistic reg_prop_task = self.params.reg_prop_tasks logit_prop_tasks = self.params.logit_prop_tasks if ( isinstance(reg_prop_task, list) and (len(reg_prop_task) > 0) and isinstance(logit_prop_tasks, list) and (len(logit_prop_tasks) > 0) ): reg_pred, logit_pred = self.property_predictor.predict(z) if isinstance(self.params.data_normalization_out_file, str): # un-normalise the predictions list # mean and std for each column of original (filtered) data. df_norm = pd.read_csv(self.params.data_normalization_out_file) reg_pred = reg_pred * df_norm[\"std\"].values + df_norm[\"mean\"].values return reg_pred, logit_pred # regression only scenario elif isinstance(reg_prop_task, list) and (len(reg_prop_task) > 0): reg_pred = self.property_predictor.predict(z) if isinstance(self.params.data_normalization_out_file, str): df_norm = pd.read_csv(self.params.data_normalization_out_file) # to watch out this flag reg_pred = reg_pred * df_norm[\"std\"].values + df_norm[\"mean\"].values return reg_pred # logit only scenario else: # unsure why it encodes z here, it seems to take it as X. Is it a bug? # probably should just pass z to the fn below, but wait to run it. logit_pred = self.property_predictor.predict(self.encode(z)) return logit_pred predict_property_function() Similar to the previous function but starts from X (3d input tensor to encoder.) SO it encodes and predicts. Source code in chemvae/vae_utils.py def predict_property_function(self): \"\"\"Similar to the previous function but starts from X (3d input tensor to encoder.) SO it encodes and predicts. \"\"\" # Now reports predictions after un-normalization. def predict_prop(X): # both regression and logistic reg_prop_task = self.params.reg_prop_tasks logit_prop_tasks = self.params.logit_prop_tasks if ( isinstance(reg_prop_task, list) and (len(reg_prop_task) > 0) and isinstance(logit_prop_tasks, list) and (len(logit_prop_tasks) > 0) ): reg_pred, logit_pred = self.property_predictor.predict(self.encode(X)) if isinstance(self.params.data_normalization_out_file, str): df_norm = pd.read_csv(self.params.data_normalization_out_file) reg_pred = reg_pred * df_norm[\"std\"].values + df_norm[\"mean\"].values return reg_pred, logit_pred # regression only scenario elif isinstance(reg_prop_task, list) and (len(reg_prop_task) > 0): reg_pred = self.property_predictor.predict(self.encode(X)) if isinstance(self.params.data_normalization_out_file, str): df_norm = pd.read_csv(self.params.data_normalization_out_file) # same, to watch out reg_pred reg_pred = reg_pred * df_norm[\"std\"].values + df_norm[\"mean\"].values return reg_pred # logit only scenario else: logit_pred = self.property_predictor.predict(self.encode(X)) return logit_pred return predict_prop prep_mol_df(smiles: list[str], z) smiles: list of smiles DECODED from perturbed versions of z z: single vector Returns: dataframe The df will have non duplicated smiles and extra colums with statistics. The way it is used is that \"smiles\" will be each smiles duplicated 25 times and perturbed, and z (z0) is the un-perturbed Now the smiles have 25 times more rows, I'm unsure how it's compensated. Source code in chemvae/vae_utils.py def prep_mol_df(self, smiles: list[str], z): \"\"\"smiles: list of smiles DECODED from perturbed versions of z z: single vector Returns: dataframe The df will have non duplicated smiles and extra colums with statistics. The way it is used is that \"smiles\" will be each smiles duplicated 25 times and perturbed, and z (z0) is the un-perturbed Now the smiles have 25 times more rows, I'm unsure how it's compensated. \"\"\" df = pd.DataFrame({\"smiles\": smiles}) # removes duplicates of perturbed-decoded string-smiles, and a count column # \"reduce\" sort_df = df.groupby(\"smiles\").size().reset_index().rename(columns={0: \"count\"}) # adds the count to each smile. df = df.merge(sort_df, on=\"smiles\") df.drop_duplicates(subset=\"smiles\", inplace=True) df = df[df[\"smiles\"].apply(mu.fast_verify)] # len counts the rows, I guess df.shape[0] is the same if not df.empty: df[\"mol\"] = df[\"smiles\"].apply(mu.smiles_to_mol) df = df[pd.notnull(df[\"mol\"])] # predicted smiles are encoded and compared initial Z smiles_list = df[\"smiles\"].tolist() df[\"distance\"] = self.smiles_distance_z(smiles_list, z) df[\"frequency\"] = df[\"count\"] / float(sum(df[\"count\"])) df = df[[\"smiles\", \"distance\", \"count\", \"frequency\", \"mol\"]] df.sort_values(by=\"distance\", inplace=True) # type: ignore df.reset_index(drop=True, inplace=True) # just to get the type. return df random_idxs(n: int) n: how many random indices to return in the new list Source code in chemvae/vae_utils.py def random_idxs(self, n: int): \"\"\"n: how many random indices to return in the new list\"\"\" return random.sample([i for i in range(len(self.smiles))], n) random_molecules(n: int) n: how many random smiles strings to return in the new list Source code in chemvae/vae_utils.py def random_molecules(self, n: int): \"\"\"n: how many random smiles strings to return in the new list\"\"\" return random.sample(self.smiles, n) smiles_distance_z(smiles: str | list[str], z0) smiles: are the predicted smiles z_rep = re-encoded & perturbed z_0 vectors (25) it's a matrix. z0 = I think is a single vector (unsure.) Returns: Distance Source code in chemvae/vae_utils.py def smiles_distance_z(self, smiles: str | list[str], z0): \"\"\"smiles: are the predicted smiles z_rep = re-encoded & perturbed z_0 vectors (25) it's a matrix. z0 = I think is a single vector (unsure.) Returns: Distance \"\"\" x = self.smiles_to_hot(smiles) # encodes again, adding perturbation (I think) and to 25 results. z_rep = self.encode(x) return np.linalg.norm(z0 - z_rep, axis=1) # 'reduce' the cols smiles_to_hot(smiles: str | list[str], canonize_smiles=False, check_smiles=False) charIndex=Indices_to_char_dict[pass_char] Make [0,...,1,...] with 1 at char index. If check_smiles=True it returns list of 'bad smiles' instead. Returns: 3D np.ndarray of hotencoded vectors Source code in chemvae/vae_utils.py def smiles_to_hot( self, smiles: str | list[str], canonize_smiles=False, check_smiles=False ): \"\"\"charIndex=Indices_to_char_dict[pass_char] Make [0,...,1,...] with 1 at char index. If check_smiles=True it returns list of 'bad smiles' instead. Returns: 3D np.ndarray of hotencoded vectors \"\"\" if isinstance(smiles, str): smiles = [smiles] if canonize_smiles: smiles = [mu.canon_smiles(s) for s in smiles] if check_smiles: smiles = mu.smiles_to_hot_filter(smiles, self.char_indices) z = mu.smiles_to_hot( smiles, self.params.MAX_LEN, self.params.PADDING, self.char_indices, self.params.NCHARS, ) return z standardize_z(z: np.ndarray) Use the mu and std vectors to normalise z Source code in chemvae/vae_utils.py def standardize_z(self, z: np.ndarray): \"\"\"Use the mu and std vectors to normalise z\"\"\" return (z - self.mu) / self.std unstandardize_z(z: np.ndarray) Use the mu and std vectors to un-normalise z Source code in chemvae/vae_utils.py def unstandardize_z(self, z: np.ndarray): \"\"\"Use the mu and std vectors to un-normalise z\"\"\" return (z * self.std) + self.mu z_to_smiles(z, decode_attempts=250, noise_norm=0.0, constant_norm=False, early_stop: int | None = None) z: single vector return: decoded smiles from 25 perturbed-vector-copies Source code in chemvae/vae_utils.py def z_to_smiles( self, z, decode_attempts=250, noise_norm=0.0, constant_norm=False, early_stop: int | None = None, ): \"\"\"z: single vector return: decoded smiles from 25 perturbed-vector-copies \"\"\" if early_stop is not None: Z = np.tile(z, (25, 1)) # copies all row vectors z 25 times # perturbed versions of the initial decoding. Z = self.perturb_z(Z, noise_norm, constant_norm) # get the character hot encoded vectors as 3D tensor X = self.decode(Z) # many decodings ! # V => character smiles = self.hot_to_smiles(X, strip=True) # remove zs decoded to same smiles and get statistics. df = self.prep_mol_df(smiles, z) if len(df) > 0: # iloc selects row, they then select column low_dist = df.iloc[0][\"distance\"] # true when closest to z is less than early stop value if low_dist < early_stop: return df # same, copy Z but this time for decoding the string z. Z = np.tile(z, (decode_attempts, 1)) Z = self.perturb_z(Z, noise_norm) X = self.decode(Z) smiles = self.hot_to_smiles(X, strip=True) df = self.prep_mol_df(smiles, z) return df Functions","title":"Vae utils.py"},{"location":"vae_utils.py/#chemvae.vae_utils-classes","text":"","title":"Classes"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils","text":"Bases: object Grabs parameters, weights, chars, creates chars <=> indices dics. encoder_file: .h5 weights file, decoder_file: .h5 weights file, Set params. Source code in chemvae/vae_utils.py def __init__( self, ): \"\"\"Set params.\"\"\" self.params = config with open(self.params.char_file) as f: chars: list[str] = yaml.safe_load(f) # json self.chars = chars self.params.NCHARS = len(self.chars) # length of charset # char <=> index self.char_indices: dict[str, int] = dict( (c, i) for i, c in enumerate(self.chars) ) self.indices_char: dict[int, str] = dict( (i, c) for i, c in enumerate(self.chars) ) # encoder, decoder, predictor self.enc = load_encoder(self.params) # model self.dec = load_decoder(self.params) # model self.encode, self.decode = self.enc_dec_functions() if self.params.do_prop_pred: self.property_predictor = load_property_predictor(self.params) self.data = None # set from file # Load path/name.csv without normalization as dataframe df, smiles_df = mu.smiles_and_full_df( self.params.data_file, self.params.MAX_LEN ) self.smiles: list[str] = smiles_df.tolist() if df.shape[1] > 1: # set everything but the smiles self.data = pd.DataFrame(df.iloc[:, 1:]) self.estimate_estandarization() return","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;VAEUtils"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils-functions","text":"","title":"Functions"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.chunks","text":"Yield successive n-sized chunks from l. indices: list of indices n: number of elements, for a slice of n elements. Source code in chemvae/vae_utils.py @staticmethod def chunks(indices: list[int], n: int): \"\"\"Yield successive n-sized chunks from l. indices: list of indices n: number of elements, for a slice of n elements. \"\"\" for i in range(0, len(indices), n): yield indices[i : i + n]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;chunks"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.enc_dec_functions","text":"Defines \"decode\" fn wrt experiment parameters. do_tgru being one of them. standardized: if z for .decode(z) is standardized. It also stands for \"standardize\" for the encode(z) function. Source code in chemvae/vae_utils.py def enc_dec_functions(self, standardized=True): \"\"\"Defines \"decode\" fn wrt experiment parameters. do_tgru being one of them. standardized: if z for .decode(z) is standardized. It also stands for \"standardize\" for the encode(z) function. \"\"\" print(\"Using standarized functions? {}\".format(standardized)) def decode(z, standardize=standardized): fake_shape = (z.shape[0], self.params.MAX_LEN, self.params.NCHARS) # no idea what this is for. fake_in = np.zeros(fake_shape) if standardize: return self.dec.predict([self.unstandardize_z(z), fake_in]) else: return self.dec.predict([z, fake_in]) def encode(X: np.ndarray, standardize=standardized): \"\"\"Encodes and optionally standardizes the resulting z. X: hot encoded matrix of smiles. standardize: whether to standardise the resulting z. \"\"\" z = self.enc.predict(X)[0] if standardize: # in this case it's not predicting? return self.standardize_z(z) else: return z return encode, decode","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;enc_dec_functions"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.estimate_estandarization","text":"After the df was split into self.smiles and self.data (un-normalised) This fn accesses self.data Source code in chemvae/vae_utils.py def estimate_estandarization(self): \"\"\"After the df was split into self.smiles and self.data (un-normalised) This fn accesses self.data \"\"\" print(\"Standarization: estimating mu and std values ...\", end=\"\") # random smiles smiles = self.random_molecules(n=50000) batch = 2500 # hidden dim is just the latent vector dimension, a hyperparameter. Z = np.zeros((len(smiles), self.params.latent_dim)) # (50000,len(z)) # [[0-2500], [2500-5000],...] iterator. for chunk in self.chunks(list(range(len(smiles))), batch): # smiles from indices sub_smiles = [smiles[i] for i in chunk] one_hot = self.smiles_to_hot(sub_smiles) # (n_smi,max_len,nchars) # each row z for smile Z[chunk, :] = self.encode(one_hot, False) # (50000, len(z)) # mean for each dimension self.mu = np.mean(Z, axis=0) # [1st col avg, 2nd col avg,...] # std for each dimension self.std = np.std(Z, axis=0) self.Z = self.standardize_z(Z) print(\"done!\") return","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;estimate_estandarization"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.hot_to_smiles","text":"From 3D tensor of Probabilities for each character & smile to smiles flat list. Source code in chemvae/vae_utils.py def hot_to_smiles(self, hot_x: Tensor3D, strip=False): \"\"\"From 3D tensor of Probabilities for each character & smile to smiles flat list. \"\"\" smiles = mu.hot_to_smiles(hot_x, self.indices_char) if strip: # postprocessing smiles = [s.strip() for s in smiles] return smiles","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;hot_to_smiles"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.ls_sampler_w_prop","text":"Returns encoded (predictions) smiles Z and corresponding data for them. Source code in chemvae/vae_utils.py def ls_sampler_w_prop(self, size=None, batch=2500, return_smiles=False): \"\"\"Returns encoded (predictions) smiles Z and corresponding data for them.\"\"\" if self.data is None: print(\"use this sampler only for external property files\") return cols = [] reg_prop_tasks = self.params.reg_prop_tasks logit_prop_tasks = self.params.logit_prop_tasks if isinstance(reg_prop_tasks, list): cols += reg_prop_tasks if isinstance(logit_prop_tasks, list): cols += logit_prop_tasks # data is the table without smiles column # note that it must be in the same order than smiles. idxs = ( self.random_idxs(size) if isinstance(size, \"int\") else list(range(len(self.smiles))) ) # grab random smiles smiles = [self.smiles[idx] for idx in idxs] # grab corresponding data data = [self.data.iloc[idx] for idx in idxs] # hidden dimensions for all input smiles Z = np.zeros((len(smiles), self.params.latent_dim)) # this avoids having more than 1 cube in memory at a time. for chunk in self.chunks(list(range(len(smiles))), batch): sub_smiles = [smiles[i] for i in chunk] # basically a small_X (3D) one_hot = self.smiles_to_hot(sub_smiles) # will slowly populate the predictions. Z[chunk, :] = self.encode(one_hot) if return_smiles: return Z, data, smiles return Z, data","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;ls_sampler_w_prop"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.perturb_z","text":"Tiny deltas to the normalised z but z here is 25 repetitions of z for a string. Source code in chemvae/vae_utils.py def perturb_z(self, z: np.ndarray, noise_norm: float, constant_norm=False): \"\"\"Tiny deltas to the normalised z but z here is 25 repetitions of z for a string. \"\"\" if noise_norm > 0.0: # n between 0 and 1 noise_vec = np.random.normal(0, 1, size=z.shape) # z (50000, 196) # F norm `sqrt(sum of the squared of all values)` # very small. noise_vec = noise_vec / np.linalg.norm(noise_vec) if constant_norm: return z + (noise_norm * noise_vec) else: # each smile a different noise scaling factor noise_amp = np.random.uniform(0, noise_norm, size=(z.shape[0], 1)) return z + (noise_amp * noise_vec) else: return z","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;perturb_z"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.predict_prop_Z","text":"Predictors from z (encoded vector) Source code in chemvae/vae_utils.py def predict_prop_Z(self, z: np.ndarray, standardized=True): \"\"\"Predictors from z (encoded vector)\"\"\" if standardized: z = self.unstandardize_z(z) # both regression and logistic reg_prop_task = self.params.reg_prop_tasks logit_prop_tasks = self.params.logit_prop_tasks if ( isinstance(reg_prop_task, list) and (len(reg_prop_task) > 0) and isinstance(logit_prop_tasks, list) and (len(logit_prop_tasks) > 0) ): reg_pred, logit_pred = self.property_predictor.predict(z) if isinstance(self.params.data_normalization_out_file, str): # un-normalise the predictions list # mean and std for each column of original (filtered) data. df_norm = pd.read_csv(self.params.data_normalization_out_file) reg_pred = reg_pred * df_norm[\"std\"].values + df_norm[\"mean\"].values return reg_pred, logit_pred # regression only scenario elif isinstance(reg_prop_task, list) and (len(reg_prop_task) > 0): reg_pred = self.property_predictor.predict(z) if isinstance(self.params.data_normalization_out_file, str): df_norm = pd.read_csv(self.params.data_normalization_out_file) # to watch out this flag reg_pred = reg_pred * df_norm[\"std\"].values + df_norm[\"mean\"].values return reg_pred # logit only scenario else: # unsure why it encodes z here, it seems to take it as X. Is it a bug? # probably should just pass z to the fn below, but wait to run it. logit_pred = self.property_predictor.predict(self.encode(z)) return logit_pred","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;predict_prop_Z"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.predict_property_function","text":"Similar to the previous function but starts from X (3d input tensor to encoder.) SO it encodes and predicts. Source code in chemvae/vae_utils.py def predict_property_function(self): \"\"\"Similar to the previous function but starts from X (3d input tensor to encoder.) SO it encodes and predicts. \"\"\" # Now reports predictions after un-normalization. def predict_prop(X): # both regression and logistic reg_prop_task = self.params.reg_prop_tasks logit_prop_tasks = self.params.logit_prop_tasks if ( isinstance(reg_prop_task, list) and (len(reg_prop_task) > 0) and isinstance(logit_prop_tasks, list) and (len(logit_prop_tasks) > 0) ): reg_pred, logit_pred = self.property_predictor.predict(self.encode(X)) if isinstance(self.params.data_normalization_out_file, str): df_norm = pd.read_csv(self.params.data_normalization_out_file) reg_pred = reg_pred * df_norm[\"std\"].values + df_norm[\"mean\"].values return reg_pred, logit_pred # regression only scenario elif isinstance(reg_prop_task, list) and (len(reg_prop_task) > 0): reg_pred = self.property_predictor.predict(self.encode(X)) if isinstance(self.params.data_normalization_out_file, str): df_norm = pd.read_csv(self.params.data_normalization_out_file) # same, to watch out reg_pred reg_pred = reg_pred * df_norm[\"std\"].values + df_norm[\"mean\"].values return reg_pred # logit only scenario else: logit_pred = self.property_predictor.predict(self.encode(X)) return logit_pred return predict_prop","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;predict_property_function"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.prep_mol_df","text":"smiles: list of smiles DECODED from perturbed versions of z z: single vector Returns: dataframe The df will have non duplicated smiles and extra colums with statistics. The way it is used is that \"smiles\" will be each smiles duplicated 25 times and perturbed, and z (z0) is the un-perturbed Now the smiles have 25 times more rows, I'm unsure how it's compensated. Source code in chemvae/vae_utils.py def prep_mol_df(self, smiles: list[str], z): \"\"\"smiles: list of smiles DECODED from perturbed versions of z z: single vector Returns: dataframe The df will have non duplicated smiles and extra colums with statistics. The way it is used is that \"smiles\" will be each smiles duplicated 25 times and perturbed, and z (z0) is the un-perturbed Now the smiles have 25 times more rows, I'm unsure how it's compensated. \"\"\" df = pd.DataFrame({\"smiles\": smiles}) # removes duplicates of perturbed-decoded string-smiles, and a count column # \"reduce\" sort_df = df.groupby(\"smiles\").size().reset_index().rename(columns={0: \"count\"}) # adds the count to each smile. df = df.merge(sort_df, on=\"smiles\") df.drop_duplicates(subset=\"smiles\", inplace=True) df = df[df[\"smiles\"].apply(mu.fast_verify)] # len counts the rows, I guess df.shape[0] is the same if not df.empty: df[\"mol\"] = df[\"smiles\"].apply(mu.smiles_to_mol) df = df[pd.notnull(df[\"mol\"])] # predicted smiles are encoded and compared initial Z smiles_list = df[\"smiles\"].tolist() df[\"distance\"] = self.smiles_distance_z(smiles_list, z) df[\"frequency\"] = df[\"count\"] / float(sum(df[\"count\"])) df = df[[\"smiles\", \"distance\", \"count\", \"frequency\", \"mol\"]] df.sort_values(by=\"distance\", inplace=True) # type: ignore df.reset_index(drop=True, inplace=True) # just to get the type. return df","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;prep_mol_df"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.random_idxs","text":"n: how many random indices to return in the new list Source code in chemvae/vae_utils.py def random_idxs(self, n: int): \"\"\"n: how many random indices to return in the new list\"\"\" return random.sample([i for i in range(len(self.smiles))], n)","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;random_idxs"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.random_molecules","text":"n: how many random smiles strings to return in the new list Source code in chemvae/vae_utils.py def random_molecules(self, n: int): \"\"\"n: how many random smiles strings to return in the new list\"\"\" return random.sample(self.smiles, n)","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;random_molecules"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.smiles_distance_z","text":"smiles: are the predicted smiles z_rep = re-encoded & perturbed z_0 vectors (25) it's a matrix. z0 = I think is a single vector (unsure.) Returns: Distance Source code in chemvae/vae_utils.py def smiles_distance_z(self, smiles: str | list[str], z0): \"\"\"smiles: are the predicted smiles z_rep = re-encoded & perturbed z_0 vectors (25) it's a matrix. z0 = I think is a single vector (unsure.) Returns: Distance \"\"\" x = self.smiles_to_hot(smiles) # encodes again, adding perturbation (I think) and to 25 results. z_rep = self.encode(x) return np.linalg.norm(z0 - z_rep, axis=1) # 'reduce' the cols","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;smiles_distance_z"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.smiles_to_hot","text":"charIndex=Indices_to_char_dict[pass_char] Make [0,...,1,...] with 1 at char index. If check_smiles=True it returns list of 'bad smiles' instead. Returns: 3D np.ndarray of hotencoded vectors Source code in chemvae/vae_utils.py def smiles_to_hot( self, smiles: str | list[str], canonize_smiles=False, check_smiles=False ): \"\"\"charIndex=Indices_to_char_dict[pass_char] Make [0,...,1,...] with 1 at char index. If check_smiles=True it returns list of 'bad smiles' instead. Returns: 3D np.ndarray of hotencoded vectors \"\"\" if isinstance(smiles, str): smiles = [smiles] if canonize_smiles: smiles = [mu.canon_smiles(s) for s in smiles] if check_smiles: smiles = mu.smiles_to_hot_filter(smiles, self.char_indices) z = mu.smiles_to_hot( smiles, self.params.MAX_LEN, self.params.PADDING, self.char_indices, self.params.NCHARS, ) return z","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;smiles_to_hot"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.standardize_z","text":"Use the mu and std vectors to normalise z Source code in chemvae/vae_utils.py def standardize_z(self, z: np.ndarray): \"\"\"Use the mu and std vectors to normalise z\"\"\" return (z - self.mu) / self.std","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;standardize_z"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.unstandardize_z","text":"Use the mu and std vectors to un-normalise z Source code in chemvae/vae_utils.py def unstandardize_z(self, z: np.ndarray): \"\"\"Use the mu and std vectors to un-normalise z\"\"\" return (z * self.std) + self.mu","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;unstandardize_z"},{"location":"vae_utils.py/#chemvae.vae_utils.VAEUtils.z_to_smiles","text":"z: single vector return: decoded smiles from 25 perturbed-vector-copies Source code in chemvae/vae_utils.py def z_to_smiles( self, z, decode_attempts=250, noise_norm=0.0, constant_norm=False, early_stop: int | None = None, ): \"\"\"z: single vector return: decoded smiles from 25 perturbed-vector-copies \"\"\" if early_stop is not None: Z = np.tile(z, (25, 1)) # copies all row vectors z 25 times # perturbed versions of the initial decoding. Z = self.perturb_z(Z, noise_norm, constant_norm) # get the character hot encoded vectors as 3D tensor X = self.decode(Z) # many decodings ! # V => character smiles = self.hot_to_smiles(X, strip=True) # remove zs decoded to same smiles and get statistics. df = self.prep_mol_df(smiles, z) if len(df) > 0: # iloc selects row, they then select column low_dist = df.iloc[0][\"distance\"] # true when closest to z is less than early stop value if low_dist < early_stop: return df # same, copy Z but this time for decoding the string z. Z = np.tile(z, (decode_attempts, 1)) Z = self.perturb_z(Z, noise_norm) X = self.decode(Z) smiles = self.hot_to_smiles(X, strip=True) df = self.prep_mol_df(smiles, z) return df","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;z_to_smiles"},{"location":"vae_utils.py/#chemvae.vae_utils-functions","text":"","title":"Functions"},{"location":"vectorize_data.py/","text":"Classes Functions vectorize_data(config: Config, do_prop_pred: bool = True) -> tuple[np.ndarray, np.ndarray] | tuple[np.ndarray, np.ndarray, list, list] Split the dataframe to: smiles_tensor[, prediction data] Returns: If true: (smiles_train, smiles_val, [reg_tasks_train, val], [log_tasks_t, v] ) If false: (smiles_train, smiles_val) The smiles are hot-encoded to a 3-D tensor. It makes sure that it's multiple of batch size, and data is randomly selected. Source code in chemvae/vectorize_data.py def vectorize_data( config: Config, do_prop_pred: bool = True ) -> tuple[np.ndarray, np.ndarray] | tuple[np.ndarray, np.ndarray, list, list]: \"\"\"Split the dataframe to: smiles_tensor[, prediction data] Returns: If true: (smiles_train, smiles_val, [reg_tasks_train, val], [log_tasks_t, v] ) If false: (smiles_train, smiles_val) The smiles are hot-encoded to a 3-D tensor. It makes sure that it's multiple of batch size, and data is randomly selected. \"\"\" MAX_LEN = config.MAX_LEN CHARS = config.CHARS NCHARS = len(CHARS) CHAR_INDICES = dict((c, i) for i, c in enumerate(CHARS)) ## Load data for properties if do_prop_pred and config.data_normalization_out_file: normalize_out = config.data_normalization_out_file else: normalize_out = None ################ matches columns in csv file ########### reg_props = config.reg_prop_tasks # list of names logit_props = config.logit_prop_tasks if do_prop_pred and not reg_props and not logit_props: raise ValueError(\"please especify logit and/or reg tasks\") # here we get the csv-data split, and optionally a \"normed write out\" for reg data. smiles, Y_reg, Y_logit = load_smiles_and_data_df( csv_file_path=config.data_file, # smiles + data max_len=MAX_LEN, reg_tasks=reg_props, logit_tasks=logit_props, normalize_out=normalize_out, # path to write normalised reg data out|None ) # subset of data if needed. if config.limit_data: # sample indices within the range. The number collected is limit_data value. sample_idx = np.random.choice( np.arange(len(smiles)), config.limit_data, replace=False ) smiles = list(np.array(smiles)[sample_idx]) # sublist of size config.limit_data if config.do_prop_pred and config.data_file: if Y_reg: Y_reg = Y_reg[sample_idx] # basically the rows of the original DF. if Y_logit: Y_logit = Y_logit[sample_idx] print(\"Training set size is\", len(smiles)) print(\"total chars:\", NCHARS) X = smiles_to_hot(smiles, MAX_LEN, config.PADDING, CHAR_INDICES, NCHARS) # if less than the batch size the `//` gives 0. if X.shape[0] % config.batch_size != 0: # make it multiple of batch_size, discard excedent. to_length = X.shape[0] // config.batch_size * config.batch_size X = X[:to_length] if config.do_prop_pred: if Y_reg: Y_reg = Y_reg[:to_length] if Y_logit: Y_logit = Y_logit[:to_length] np.random.seed(config.RAND_SEED) rand_idx = np.arange(X.shape[0]) np.random.shuffle(rand_idx) # shuffles the rows' indices. TRAIN_FRAC = 1 - config.val_split num_train = int(X.shape[0] * TRAIN_FRAC) # or gets 0 if num_train % config.batch_size != 0: num_train = num_train // config.batch_size * config.batch_size print(\"num_train \", num_train) # makes the indices for each train_idx, test_idx = rand_idx[: int(num_train)], rand_idx[int(num_train) :] if config.test_idx_file: np.save(config.test_idx_file, test_idx) # grab the rows with an list of random indices. X_train, X_test = X[train_idx], X[test_idx] print(f\"shape of training input vector : {X_train.shape}\") if do_prop_pred: # !# add Y_train and Y_test here Y_train = [] Y_test = [] if Y_reg: Y_reg_train, Y_reg_test = Y_reg[train_idx], Y_reg[test_idx] Y_train.append(Y_reg_train) Y_test.append(Y_reg_test) if Y_logit: Y_logit_train, Y_logit_test = Y_logit[train_idx], Y_logit[test_idx] Y_train.append(Y_logit_train) Y_test.append(Y_logit_test) return X_train, X_test, Y_train, Y_test else: return X_train, X_test","title":"Vectorize data.py"},{"location":"vectorize_data.py/#chemvae.vectorize_data-classes","text":"","title":"Classes"},{"location":"vectorize_data.py/#chemvae.vectorize_data-functions","text":"","title":"Functions"},{"location":"vectorize_data.py/#chemvae.vectorize_data.vectorize_data","text":"Split the dataframe to: smiles_tensor[, prediction data] Returns: If true: (smiles_train, smiles_val, [reg_tasks_train, val], [log_tasks_t, v] ) If false: (smiles_train, smiles_val) The smiles are hot-encoded to a 3-D tensor. It makes sure that it's multiple of batch size, and data is randomly selected. Source code in chemvae/vectorize_data.py def vectorize_data( config: Config, do_prop_pred: bool = True ) -> tuple[np.ndarray, np.ndarray] | tuple[np.ndarray, np.ndarray, list, list]: \"\"\"Split the dataframe to: smiles_tensor[, prediction data] Returns: If true: (smiles_train, smiles_val, [reg_tasks_train, val], [log_tasks_t, v] ) If false: (smiles_train, smiles_val) The smiles are hot-encoded to a 3-D tensor. It makes sure that it's multiple of batch size, and data is randomly selected. \"\"\" MAX_LEN = config.MAX_LEN CHARS = config.CHARS NCHARS = len(CHARS) CHAR_INDICES = dict((c, i) for i, c in enumerate(CHARS)) ## Load data for properties if do_prop_pred and config.data_normalization_out_file: normalize_out = config.data_normalization_out_file else: normalize_out = None ################ matches columns in csv file ########### reg_props = config.reg_prop_tasks # list of names logit_props = config.logit_prop_tasks if do_prop_pred and not reg_props and not logit_props: raise ValueError(\"please especify logit and/or reg tasks\") # here we get the csv-data split, and optionally a \"normed write out\" for reg data. smiles, Y_reg, Y_logit = load_smiles_and_data_df( csv_file_path=config.data_file, # smiles + data max_len=MAX_LEN, reg_tasks=reg_props, logit_tasks=logit_props, normalize_out=normalize_out, # path to write normalised reg data out|None ) # subset of data if needed. if config.limit_data: # sample indices within the range. The number collected is limit_data value. sample_idx = np.random.choice( np.arange(len(smiles)), config.limit_data, replace=False ) smiles = list(np.array(smiles)[sample_idx]) # sublist of size config.limit_data if config.do_prop_pred and config.data_file: if Y_reg: Y_reg = Y_reg[sample_idx] # basically the rows of the original DF. if Y_logit: Y_logit = Y_logit[sample_idx] print(\"Training set size is\", len(smiles)) print(\"total chars:\", NCHARS) X = smiles_to_hot(smiles, MAX_LEN, config.PADDING, CHAR_INDICES, NCHARS) # if less than the batch size the `//` gives 0. if X.shape[0] % config.batch_size != 0: # make it multiple of batch_size, discard excedent. to_length = X.shape[0] // config.batch_size * config.batch_size X = X[:to_length] if config.do_prop_pred: if Y_reg: Y_reg = Y_reg[:to_length] if Y_logit: Y_logit = Y_logit[:to_length] np.random.seed(config.RAND_SEED) rand_idx = np.arange(X.shape[0]) np.random.shuffle(rand_idx) # shuffles the rows' indices. TRAIN_FRAC = 1 - config.val_split num_train = int(X.shape[0] * TRAIN_FRAC) # or gets 0 if num_train % config.batch_size != 0: num_train = num_train // config.batch_size * config.batch_size print(\"num_train \", num_train) # makes the indices for each train_idx, test_idx = rand_idx[: int(num_train)], rand_idx[int(num_train) :] if config.test_idx_file: np.save(config.test_idx_file, test_idx) # grab the rows with an list of random indices. X_train, X_test = X[train_idx], X[test_idx] print(f\"shape of training input vector : {X_train.shape}\") if do_prop_pred: # !# add Y_train and Y_test here Y_train = [] Y_test = [] if Y_reg: Y_reg_train, Y_reg_test = Y_reg[train_idx], Y_reg[test_idx] Y_train.append(Y_reg_train) Y_test.append(Y_reg_test) if Y_logit: Y_logit_train, Y_logit_test = Y_logit[train_idx], Y_logit[test_idx] Y_train.append(Y_logit_train) Y_test.append(Y_logit_test) return X_train, X_test, Y_train, Y_test else: return X_train, X_test","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;vectorize_data"}]}